{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I7je3PXR6IOO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "    return train_loader, test_loader\n"
      ],
      "metadata": {
        "id": "hih3jnYYQP1J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5gkmtZDbQVe9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    return running_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "    return running_loss / len(test_loader), 100. * correct / total, classification_report(all_targets, all_predictions)\n"
      ],
      "metadata": {
        "id": "u1EMjOMpQbeY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "train_loader, test_loader = prepare_data()\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc, report = evaluate_model(model, test_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} -> Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "torch.save(model.state_dict(), \"cnnwithlatency_model.pth\")\n",
        "print(\"Model saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RYCket0uQe5U",
        "outputId": "fe861a3f-0128-41e7-8fed-92414707f08d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 55.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch 1/10 -> Train Loss: 1.6080, Train Acc: 40.54%, Test Loss: 1.1786, Test Acc: 57.55%\n",
            "Epoch 2/10 -> Train Loss: 1.2299, Train Acc: 56.07%, Test Loss: 0.9890, Test Acc: 65.14%\n",
            "Epoch 3/10 -> Train Loss: 1.0702, Train Acc: 62.25%, Test Loss: 0.9027, Test Acc: 68.17%\n",
            "Epoch 4/10 -> Train Loss: 0.9681, Train Acc: 65.96%, Test Loss: 0.8197, Test Acc: 70.73%\n",
            "Epoch 5/10 -> Train Loss: 0.9032, Train Acc: 68.48%, Test Loss: 0.7817, Test Acc: 72.75%\n",
            "Epoch 6/10 -> Train Loss: 0.8547, Train Acc: 70.18%, Test Loss: 0.7391, Test Acc: 73.91%\n",
            "Epoch 7/10 -> Train Loss: 0.8204, Train Acc: 71.48%, Test Loss: 0.7158, Test Acc: 75.30%\n",
            "Epoch 8/10 -> Train Loss: 0.7858, Train Acc: 72.95%, Test Loss: 0.6719, Test Acc: 76.47%\n",
            "Epoch 9/10 -> Train Loss: 0.7654, Train Acc: 73.67%, Test Loss: 0.6555, Test Acc: 77.42%\n",
            "Epoch 10/10 -> Train Loss: 0.7371, Train Acc: 74.60%, Test Loss: 0.6412, Test Acc: 77.81%\n",
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(device)\n",
        "model.load_state_dict(torch.load(\"cnnwithlatency_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "def measure_latency(model, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "    if device == 'cuda':\n",
        "        start_event = torch.cuda.Event(enable_timing=True)\n",
        "        end_event = torch.cuda.Event(enable_timing=True)\n",
        "        start_event.record()\n",
        "        model(dummy_input)\n",
        "        end_event.record()\n",
        "        torch.cuda.synchronize()\n",
        "        latency = start_event.elapsed_time(end_event)\n",
        "        print(f\"GPU Latency: {latency:.2f} ms\")\n",
        "    else:\n",
        "        start_time = time.time()\n",
        "        model(dummy_input)\n",
        "        end_time = time.time()\n",
        "        latency = (end_time - start_time) * 1000\n",
        "        print(f\"CPU Latency: {latency:.2f} ms\")\n",
        "\n",
        "measure_latency(model, 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    measure_latency(model, 'cuda')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "i_XlCaCdQk3l",
        "outputId": "e0e0775a-929d-4b67-dc93-709d1bd9fa9d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-f4e00528cf0c>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"cnnwithlatency_model.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Latency: 56.53 ms\n",
            "GPU Latency: 141.10 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "w4TQ2aZ3Rr1o",
        "outputId": "3bcb8414-8cea-40d3-f496-0b1175de965e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.4M\n",
            "-rw-r--r-- 1 root root 2.4M Dec 31 10:32 cnnwithlatency_model.pth\n",
            "drwxr-xr-x 3 root root 4.0K Dec 31 10:28 data\n",
            "drwxr-xr-x 1 root root 4.0K Dec 19 14:20 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fmdamRXMSc8a",
        "outputId": "d27d6560-51b5-4ea8-d629-01d582d1bf04"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 3, 32, 32).to(device)  # Example input for the model\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"cnnwithlatency_model.onnx\",  # File name for ONNX\n",
        "    input_names=[\"input\"],  # Name of input layer\n",
        "    output_names=[\"output\"],  # Name of output layer\n",
        "    opset_version=11  # ONNX opset version\n",
        ")\n",
        "print(\"ONNX model exported as 'cnnwithlatency_model.onnx'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gsykFAMfS-jD",
        "outputId": "90ebc3bb-98df-42f5-a0fd-176744e62807"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model exported as 'cnnwithlatency_model.onnx'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the ONNX model\n",
        "files.download(\"cnnwithlatency_model.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "SadPhonuWjdi",
        "outputId": "ce4c5da3-8bfe-43c4-d791-99d97b768c3d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e7e1acb9-e4f4-48dc-b4bf-33cb44a2e836\", \"cnnwithlatency_model.onnx\", 2484185)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vxq0BUHvWrTX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}