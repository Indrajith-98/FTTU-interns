{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-sEPq-3xlSZ",
        "outputId": "323b281d-5ad3-440d-ef29-71b3e2306757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Tue Jan 21 13:35:43 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [62.5 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,234 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,606 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,228 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,519 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,561 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,860 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,641 kB]\n",
            "Fetched 21.1 MB in 2s (9,366 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libnvinfer-headers-dev libnvinfer10\n",
            "The following NEW packages will be installed:\n",
            "  libnvinfer-dev libnvinfer-headers-dev libnvinfer-plugin8 libnvinfer10\n",
            "  libnvinfer8\n",
            "0 upgraded, 5 newly installed, 0 to remove and 65 not upgraded.\n",
            "Need to get 2,989 MB of archives.\n",
            "After this operation, 7,707 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-headers-dev 10.7.0.23-1+cuda12.6 [106 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer10 10.7.0.23-1+cuda12.6 [1,240 MB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dev 10.7.0.23-1+cuda12.6 [1,246 MB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer8 8.6.1.6-1+cuda12.0 [492 MB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-plugin8 8.6.1.6-1+cuda12.0 [11.7 MB]\n",
            "Fetched 2,989 MB in 1min 52s (26.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libnvinfer-headers-dev.\n",
            "(Reading database ... 124565 files and directories currently installed.)\n",
            "Preparing to unpack .../libnvinfer-headers-dev_10.7.0.23-1+cuda12.6_amd64.deb ...\n",
            "Unpacking libnvinfer-headers-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Selecting previously unselected package libnvinfer10.\n",
            "Preparing to unpack .../libnvinfer10_10.7.0.23-1+cuda12.6_amd64.deb ...\n",
            "Unpacking libnvinfer10 (10.7.0.23-1+cuda12.6) ...\n",
            "Selecting previously unselected package libnvinfer-dev.\n",
            "Preparing to unpack .../libnvinfer-dev_10.7.0.23-1+cuda12.6_amd64.deb ...\n",
            "Unpacking libnvinfer-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Selecting previously unselected package libnvinfer8.\n",
            "Preparing to unpack .../libnvinfer8_8.6.1.6-1+cuda12.0_amd64.deb ...\n",
            "Unpacking libnvinfer8 (8.6.1.6-1+cuda12.0) ...\n",
            "Selecting previously unselected package libnvinfer-plugin8.\n",
            "Preparing to unpack .../libnvinfer-plugin8_8.6.1.6-1+cuda12.0_amd64.deb ...\n",
            "Unpacking libnvinfer-plugin8 (8.6.1.6-1+cuda12.0) ...\n",
            "Setting up libnvinfer-headers-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Setting up libnvinfer10 (10.7.0.23-1+cuda12.6) ...\n",
            "Setting up libnvinfer-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Setting up libnvinfer8 (8.6.1.6-1+cuda12.0) ...\n",
            "Setting up libnvinfer-plugin8 (8.6.1.6-1+cuda12.0) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "Cloning into 'tensorflow'...\n",
            "remote: Enumerating objects: 1955956, done.\u001b[K\n",
            "remote: Counting objects: 100% (354/354), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 1955956 (delta 234), reused 165 (delta 156), pack-reused 1955602 (from 5)\u001b[K\n",
            "Receiving objects: 100% (1955956/1955956), 1.06 GiB | 26.92 MiB/s, done.\n",
            "Resolving deltas: 100% (1609083/1609083), done.\n",
            "Updating files: 100% (34651/34651), done.\n",
            "/content/tensorflow\n",
            "Updating files: 100% (28206/28206), done.\n",
            "Branch 'r2.10' set up to track remote branch 'r2.10' from 'origin'.\n",
            "Switched to a new branch 'r2.10'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bazel build --config=cuda --config=monolithic ... (Specify the build target with TensorRT support)'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bazel install ... (Install the built TensorFlow package)'\n"
          ]
        }
      ],
      "source": [
        "# Ensure CUDA and cuDNN are installed\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "# Install the required dependencies for building TensorFlow with TensorRT support\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y libnvinfer8 libnvinfer-dev libnvinfer-plugin8\n",
        "# (Install other necessary packages as mentioned in TensorFlow documentation)\n",
        "# Clone the TensorFlow repository and checkout the desired branch\n",
        "!git clone https://github.com/tensorflow/tensorflow.git\n",
        "%cd tensorflow\n",
        "!git checkout r2.10 # Check the TensorFlow-TensorRT compatibility matrix for the correct branch.\n",
        "# Configure TensorFlow build with TensorRT enabled\n",
        "# ./configure\n",
        "# (During configuration, enable TensorRT support when prompted)\n",
        "# If you are using a virtual environment, activate it before building TensorFlow.\n",
        "# Build and install TensorFlow\n",
        "!bazel build --config=cuda --config=monolithic ... (Specify the build target with TensorRT support)\n",
        "!bazel install ... (Install the built TensorFlow package)\n",
        "# After successful installation, restart the runtime to ensure the new TensorFlow installation is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dvcSHgfz8m9",
        "outputId": "2959562f-f1eb-41b3-dd42-84a25a53f10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 21 13:45:59 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "MwutDdg51rgy",
        "outputId": "c7364687-beec-4a96-cfc7-e3b752042106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (1.26.4)\n",
            "Collecting onnx>=1.4.1 (from tf2onnx)\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (24.12.23)\n",
            "Collecting protobuf~=3.20 (from tf2onnx)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2024.12.14)\n",
            "Downloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, onnx, tf2onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.17.0 protobuf-3.20.3 tf2onnx-1.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "b13ef06db6974ac19ea00769668bdc0a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tf2onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNXWqRfj3yct",
        "outputId": "f6291ce2-b08d-44b1-8034-e6d55ecee4f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n"
          ]
        }
      ],
      "source": [
        "%cd ~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YsuzceQHnfF1"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kWWCU3k2nwaH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6R-SHiqunye5"
      },
      "outputs": [],
      "source": [
        "def convert_keras_to_tensorrt(keras_model_path, trt_model_dir):\n",
        "    \"\"\"\n",
        "    Converts a Keras model to a TensorRT-optimized SavedModel.\n",
        "\n",
        "    Args:\n",
        "        keras_model_path (str): Path to the .keras model file.\n",
        "        trt_model_dir (str): Directory where the TensorRT-optimized model will be saved.\n",
        "    \"\"\"\n",
        "    print(\"Loading Keras model...\")\n",
        "    # Load the Keras model\n",
        "    keras_model = tf.keras.models.load_model(keras_model_path)\n",
        "\n",
        "    # Create a temporary directory to save the SavedModel\n",
        "    temp_saved_model_dir = \"temp_saved_model\"\n",
        "    os.makedirs(temp_saved_model_dir, exist_ok=True)\n",
        "\n",
        "    # Save the Keras model as a SavedModel\n",
        "    print(\"Saving Keras model as SavedModel...\")\n",
        "    tf.saved_model.save(keras_model, temp_saved_model_dir)\n",
        "\n",
        "    print(\"Converting Keras model to TensorRT...\")\n",
        "    # Initialize the TensorRT converter\n",
        "    params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
        "        precision_mode=trt.TrtPrecisionMode.FP16,  # Use FP16 for faster inference (if supported)\n",
        "        max_workspace_size_bytes=1 << 30          # 1GB workspace size\n",
        "    )\n",
        "    # Pass the temporary SavedModel directory to the converter\n",
        "    converter = trt.TrtGraphConverterV2(input_saved_model_dir=temp_saved_model_dir, conversion_params=params)\n",
        "\n",
        "    # Convert the Keras model\n",
        "    converter.convert()\n",
        "\n",
        "    # Save the TensorRT-optimized model\n",
        "    print(f\"Saving TensorRT-optimized model to {trt_model_dir}...\")\n",
        "    converter.save(trt_model_dir)\n",
        "    print(f\"TensorRT-optimized model saved at {trt_model_dir}\")\n",
        "\n",
        "    # Optionally remove the temporary SavedModel directory\n",
        "    # import shutil\n",
        "    # shutil.rmtree(temp_saved_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4r7hdfErn1P4"
      },
      "outputs": [],
      "source": [
        "def load_tensorrt_model_and_infer(trt_model_dir, input_data):\n",
        "    \"\"\"\n",
        "    Loads a TensorRT-optimized model and performs inference.\n",
        "\n",
        "    Args:\n",
        "        trt_model_dir (str): Directory of the TensorRT-optimized model.\n",
        "        input_data (numpy.ndarray): Input data for inference.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Model predictions.\n",
        "    \"\"\"\n",
        "    print(\"Loading TensorRT-optimized model...\")\n",
        "    trt_model = tf.saved_model.load(trt_model_dir)\n",
        "    infer = trt_model.signatures[\"serving_default\"]\n",
        "\n",
        "    print(\"Running inference...\")\n",
        "    # Perform inference\n",
        "    predictions = infer(tf.convert_to_tensor(input_data))\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JW9pFe86n4Kq"
      },
      "outputs": [],
      "source": [
        "keras_model_path = \"/content/drive/MyDrive/MNIST_H5_model/lightweight_resnet_mnist.h5\"         # Path to your .keras model\n",
        "trt_model_dir = \"/content/drive/MyDrive/MNIST_H5_model/TRT_Model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZJSnu3vpGwE",
        "outputId": "9e05eb65-de02-4acd-f6f9-6f53fca1c5c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Keras model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Keras model as SavedModel...\n",
            "Converting Keras model to TensorRT...\n",
            "Saving TensorRT-optimized model to /content/drive/MyDrive/MNIST_H5_model/TRT_Model...\n",
            "TensorRT-optimized model saved at /content/drive/MyDrive/MNIST_H5_model/TRT_Model\n"
          ]
        }
      ],
      "source": [
        "convert_keras_to_tensorrt(keras_model_path, trt_model_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}