{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUiC2IgJlXSd",
        "outputId": "b0562e8e-e49d-4fca-fd2d-d2d6816e76ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_tuner in /usr/local/lib/python3.11/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras_tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "W74Uctc_lYuT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Function to preprocess a single image\n",
        "def preprocess_image(image, target_size=(64, 64)):  # Smaller input size to reduce memory usage\n",
        "    image = tf.expand_dims(image, axis=-1)  # Add channel dimension\n",
        "    image = tf.image.grayscale_to_rgb(image)  # Convert to RGB\n",
        "    image = tf.image.resize(image, target_size)  # Resize image\n",
        "    return image / 255.0  # Normalize\n",
        "\n",
        "# Convert dataset into TensorFlow Dataset objects\n",
        "batch_size = 16  # Smaller batch size to reduce memory consumption\n",
        "target_size = (64, 64)  # Smaller target size\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.map(lambda x, y: (preprocess_image(x, target_size), tf.one_hot(y, 10)))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.map(lambda x, y: (preprocess_image(x, target_size), tf.one_hot(y, 10)))\n",
        "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Lightweight ResNet Block\n",
        "def resnet_block(x, filters, stride=1, dropout_rate=0.2):  # Reduced dropout to balance memory and regularization\n",
        "    shortcut = x\n",
        "\n",
        "    # First convolution\n",
        "    x = Conv2D(filters, (3, 3), strides=stride, padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    # Second convolution\n",
        "    x = Conv2D(filters, (3, 3), strides=1, padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Adjust shortcut dimensions if necessary\n",
        "    if stride != 1 or x.shape[-1] != shortcut.shape[-1]:\n",
        "        shortcut = Conv2D(filters, (1, 1), strides=stride, use_bias=False)(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "    x = Add()([x, shortcut])\n",
        "    x = ReLU()(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "# Build the Lightweight ResNet Model\n",
        "def build_lightweight_resnet(hp, input_shape=(64, 64, 3), num_classes=10):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Initial Conv Layer\n",
        "    x = Conv2D(hp.Int('initial_filters', min_value=16, max_value=32, step=8),\n",
        "               (3, 3), strides=1, padding='same', use_bias=False)(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    # Residual Blocks with reduced filters\n",
        "    for filters in [32, 64]:  # Fewer filters to save memory\n",
        "        x = resnet_block(x, filters, stride=2, dropout_rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.4, step=0.1))\n",
        "\n",
        "    # Global Pooling and Output Layer\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-3, sampling='log')\n",
        "        ),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Hyperparameter Tuning with Keras Tuner\n",
        "def tune_hyperparameters():\n",
        "    tuner = kt.RandomSearch(  # RandomSearch for quicker results\n",
        "        build_lightweight_resnet,\n",
        "        objective='val_accuracy',\n",
        "        max_trials=5,  # Reduce the number of trials\n",
        "        directory='tuner',\n",
        "        project_name='lightweight_resnet_mnist_optimized'\n",
        "    )\n",
        "\n",
        "    # Train the model with Keras Tuner\n",
        "    tuner.search(train_dataset, validation_data=test_dataset, epochs=3)  # Fewer epochs to save memory and time\n",
        "\n",
        "    # Retrieve the best model and hyperparameters\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "    best_hps = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters\n",
        "\n",
        "    return best_model, best_hps\n"
      ],
      "metadata": {
        "id": "Qh1r8gg7lgN7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, best_hps = tune_hyperparameters()\n",
        "\n",
        "# Show the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_hps.values)\n",
        "\n",
        "# Final Evaluation\n",
        "final_loss, final_accuracy = best_model.evaluate(test_dataset, verbose=2)\n",
        "print(f\"Final Test Loss: {final_loss:.4f}\")\n",
        "print(f\"Final Test Accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "# Save the best model\n",
        "saved_model_dir = \"/content/lightweight_resnet_mnist_optimized.h5\"\n",
        "best_model.save(saved_model_dir)\n",
        "print(f\"Best Model saved to {saved_model_dir}. You can now download it manually.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C8zsKxdlkmK",
        "outputId": "0928128d-eba5-48b0-c96c-a31639ecaa14"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 01m 36s]\n",
            "val_accuracy: 0.9732999801635742\n",
            "\n",
            "Best val_accuracy So Far: 0.9732999801635742\n",
            "Total elapsed time: 00h 08m 04s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 48 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'initial_filters': 24, 'dropout_rate': 0.4, 'learning_rate': 0.0009287197673218198}\n",
            "625/625 - 3s - 5ms/step - accuracy: 0.9733 - loss: 0.0882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Loss: 0.0882\n",
            "Final Test Accuracy: 0.9733\n",
            "Best Model saved to /content/lightweight_resnet_mnist_optimized.h5. You can now download it manually.\n"
          ]
        }
      ]
    }
  ]
}