{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import graph_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /home/rakshan/tvm/build/tvm_env/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/rakshan/tvm/build/tvm_env/lib/python3.12/site-packages (from onnx) (2.2.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/rakshan/tvm/build/tvm_env/lib/python3.12/site-packages (from onnx) (5.29.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "  Name: input\n",
      "  Shape: [None, 112, 112, 3]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Path to your ONNX model\n",
    "onnx_model_path = \"/home/rakshan/TVM_assignment/onnx_lightweight_resnet_mnist.onnx\"\n",
    "\n",
    "# Load ONNX model\n",
    "model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Check the input information\n",
    "print(\"Inputs:\")\n",
    "for input_tensor in model.graph.input:\n",
    "    name = input_tensor.name\n",
    "    type_proto = input_tensor.type\n",
    "    shape = None\n",
    "    \n",
    "    # Try to extract the shape if available\n",
    "    if type_proto.HasField(\"tensor_type\"):\n",
    "        tensor_type = type_proto.tensor_type\n",
    "        if tensor_type.HasField(\"shape\"):\n",
    "            dims = tensor_type.shape.dim\n",
    "            shape = [dim.dim_value if dim.HasField(\"dim_value\") else None for dim in dims]\n",
    "    \n",
    "    print(f\"  Name: {name}\")\n",
    "    print(f\"  Shape: {shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your ONNX model\n",
    "onnx_model_path = \"/home/rakshan/TVM_assignment/onnx_lightweight_resnet_mnist.onnx\"\n",
    "\n",
    "# Load ONNX model\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Define the target device\n",
    "target = \"llvm\"  # Change to \"cuda\" for GPU\n",
    "\n",
    "# Correct input name and shape\n",
    "input_name = \"input\"  # Update with the correct input name\n",
    "input_shape = (1, 112, 112, 3)  # Define a fixed batch size, replacing None with 1\n",
    "\n",
    "# Convert ONNX model to Relay IR\n",
    "shape_dict = {input_name: input_shape}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "# Optimize the model using TVM\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# Export the compiled library\n",
    "lib.export_library(\"optimized_model.so\")\n",
    "\n",
    "# Load the optimized module for testing or deployment\n",
    "dev = tvm.device(target, 0)\n",
    "module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n",
    "# Example of running inference\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample input array with the appropriate shape\n",
    "input_data = np.random.rand(*input_shape).astype(\"float32\")\n",
    "module.set_input(input_name, input_data)\n",
    "module.run()\n",
    "\n",
    "# Get output and print results\n",
    "output_data = module.get_output(0).asnumpy()\n",
    "print(\"Output shape:\", output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_json\",\"w\") as f:\n",
    "    f.write(lib.get_graph_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
