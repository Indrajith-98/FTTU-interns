{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee39ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68bdd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Save tensor as binary file\n",
    "def save_tensor_as_bin(tensor, filepath):\n",
    "    np_array = tensor.numpy() if isinstance(tensor, tf.Tensor) else tensor\n",
    "    np_array.tofile(filepath)\n",
    "\n",
    "# Save JSON file\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Create directories\n",
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea216cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer-wise dump with a single JSON for the model architecture\n",
    "def layer_wise_dump_tf(model, input_tensor, save_dir):\n",
    "    # Ensure save directory structure\n",
    "    create_dir(save_dir)\n",
    "    create_dir(f\"{save_dir}/input\")\n",
    "    create_dir(f\"{save_dir}/output\")\n",
    "    create_dir(f\"{save_dir}/weights\")\n",
    "    create_dir(f\"{save_dir}/reference\")\n",
    "\n",
    "    model_architecture = {\n",
    "        \"model_name\": model.name,\n",
    "        \"num_layers\": len(model.layers),\n",
    "        \"input_shape\": list(model.input_shape) if hasattr(model, 'input_shape') else \"Unknown\",\n",
    "        \"output_shape\": list(model.output_shape) if hasattr(model, 'output_shape') else \"Unknown\",\n",
    "        \"layers\": []\n",
    "    }\n",
    "\n",
    "    x = input_tensor\n",
    "    for layer in model.layers:\n",
    "        layer_name = layer.name\n",
    "\n",
    "        # Save input tensor\n",
    "        input_filepath = f\"{save_dir}/input/{layer_name}_input.bin\"\n",
    "        save_tensor_as_bin(x, input_filepath)\n",
    "\n",
    "        # Save weights\n",
    "        weight_filepaths = []\n",
    "        if hasattr(layer, 'weights') and layer.weights:\n",
    "            for weight in layer.weights:\n",
    "                weight_name = weight.name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "                weight_filepath = f\"{save_dir}/weights/{layer_name}_{weight_name}.bin\"\n",
    "                save_tensor_as_bin(weight.numpy(), weight_filepath)\n",
    "                weight_filepaths.append(weight_filepath)\n",
    "\n",
    "        # Compute layer output\n",
    "        x = layer(x)\n",
    "\n",
    "        # Save output tensor\n",
    "        output_filepath = f\"{save_dir}/output/{layer_name}_output.bin\"\n",
    "        save_tensor_as_bin(x, output_filepath)\n",
    "\n",
    "        # Gather layer attributes\n",
    "        layer_attributes = {\n",
    "            \"layer_name\": layer_name,\n",
    "            \"type\": layer.__class__.__name__,\n",
    "            \"input_shape\": list(layer.input_shape) if hasattr(layer, 'input_shape') else \"Unknown\",\n",
    "            \"output_shape\": list(layer.output_shape) if hasattr(layer, 'output_shape') else \"Unknown\",\n",
    "            \"input_file\": input_filepath,\n",
    "            \"output_file\": output_filepath,\n",
    "            \"weight_files\": weight_filepaths,\n",
    "        }\n",
    "        if hasattr(layer, 'kernel_size'):\n",
    "            layer_attributes[\"kernel_size\"] = layer.kernel_size\n",
    "        if hasattr(layer, 'strides'):\n",
    "            layer_attributes[\"strides\"] = layer.strides\n",
    "        if hasattr(layer, 'padding'):\n",
    "            layer_attributes[\"padding\"] = layer.padding\n",
    "        if hasattr(layer, 'activation') and layer.activation:\n",
    "            layer_attributes[\"activation\"] = layer.activation.__name__\n",
    "\n",
    "        # Add layer attributes to the model architecture JSON\n",
    "        model_architecture[\"layers\"].append(layer_attributes)\n",
    "\n",
    "    # Save the complete model architecture to a single JSON file\n",
    "    save_json(model_architecture, f\"{save_dir}/model_architecture.json\")\n",
    "\n",
    "    return model_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9bfb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"C:/Users/HP/Operators_Assignment/cnn_cifar10_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "328d7003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'sequential',\n",
       " 'num_layers': 8,\n",
       " 'input_shape': [None, 32, 32, 3],\n",
       " 'output_shape': [None, 10],\n",
       " 'layers': [{'layer_name': 'conv2d',\n",
       "   'type': 'Conv2D',\n",
       "   'input_shape': [None, 32, 32, 3],\n",
       "   'output_shape': [None, 30, 30, 32],\n",
       "   'input_file': 'data/input/conv2d_input.bin',\n",
       "   'output_file': 'data/output/conv2d_output.bin',\n",
       "   'weight_files': ['data/weights/conv2d_kernel_0.bin',\n",
       "    'data/weights/conv2d_bias_0.bin'],\n",
       "   'kernel_size': (3, 3),\n",
       "   'strides': (1, 1),\n",
       "   'padding': 'valid',\n",
       "   'activation': 'relu'},\n",
       "  {'layer_name': 'max_pooling2d',\n",
       "   'type': 'MaxPooling2D',\n",
       "   'input_shape': [None, 30, 30, 32],\n",
       "   'output_shape': [None, 15, 15, 32],\n",
       "   'input_file': 'data/input/max_pooling2d_input.bin',\n",
       "   'output_file': 'data/output/max_pooling2d_output.bin',\n",
       "   'weight_files': [],\n",
       "   'strides': (2, 2),\n",
       "   'padding': 'valid'},\n",
       "  {'layer_name': 'conv2d_1',\n",
       "   'type': 'Conv2D',\n",
       "   'input_shape': [None, 15, 15, 32],\n",
       "   'output_shape': [None, 13, 13, 64],\n",
       "   'input_file': 'data/input/conv2d_1_input.bin',\n",
       "   'output_file': 'data/output/conv2d_1_output.bin',\n",
       "   'weight_files': ['data/weights/conv2d_1_kernel_0.bin',\n",
       "    'data/weights/conv2d_1_bias_0.bin'],\n",
       "   'kernel_size': (3, 3),\n",
       "   'strides': (1, 1),\n",
       "   'padding': 'valid',\n",
       "   'activation': 'relu'},\n",
       "  {'layer_name': 'max_pooling2d_1',\n",
       "   'type': 'MaxPooling2D',\n",
       "   'input_shape': [None, 13, 13, 64],\n",
       "   'output_shape': [None, 6, 6, 64],\n",
       "   'input_file': 'data/input/max_pooling2d_1_input.bin',\n",
       "   'output_file': 'data/output/max_pooling2d_1_output.bin',\n",
       "   'weight_files': [],\n",
       "   'strides': (2, 2),\n",
       "   'padding': 'valid'},\n",
       "  {'layer_name': 'conv2d_2',\n",
       "   'type': 'Conv2D',\n",
       "   'input_shape': [None, 6, 6, 64],\n",
       "   'output_shape': [None, 4, 4, 128],\n",
       "   'input_file': 'data/input/conv2d_2_input.bin',\n",
       "   'output_file': 'data/output/conv2d_2_output.bin',\n",
       "   'weight_files': ['data/weights/conv2d_2_kernel_0.bin',\n",
       "    'data/weights/conv2d_2_bias_0.bin'],\n",
       "   'kernel_size': (3, 3),\n",
       "   'strides': (1, 1),\n",
       "   'padding': 'valid',\n",
       "   'activation': 'relu'},\n",
       "  {'layer_name': 'flatten',\n",
       "   'type': 'Flatten',\n",
       "   'input_shape': [None, 4, 4, 128],\n",
       "   'output_shape': [None, 2048],\n",
       "   'input_file': 'data/input/flatten_input.bin',\n",
       "   'output_file': 'data/output/flatten_output.bin',\n",
       "   'weight_files': []},\n",
       "  {'layer_name': 'dense',\n",
       "   'type': 'Dense',\n",
       "   'input_shape': [None, 2048],\n",
       "   'output_shape': [None, 128],\n",
       "   'input_file': 'data/input/dense_input.bin',\n",
       "   'output_file': 'data/output/dense_output.bin',\n",
       "   'weight_files': ['data/weights/dense_kernel_0.bin',\n",
       "    'data/weights/dense_bias_0.bin'],\n",
       "   'activation': 'relu'},\n",
       "  {'layer_name': 'dense_1',\n",
       "   'type': 'Dense',\n",
       "   'input_shape': [None, 128],\n",
       "   'output_shape': [None, 10],\n",
       "   'input_file': 'data/input/dense_1_input.bin',\n",
       "   'output_file': 'data/output/dense_1_output.bin',\n",
       "   'weight_files': ['data/weights/dense_1_kernel_0.bin',\n",
       "    'data/weights/dense_1_bias_0.bin'],\n",
       "   'activation': 'softmax'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = tf.random.normal([1, 32, 32, 3])  # Random input for CIFAR-10\n",
    "layer_wise_dump_tf(model, input_tensor, save_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0197016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def generate_layer_config(model, save_dir, config_file_path):\n",
    "    # Build model or run a dummy forward pass to infer shapes\n",
    "    if not model.built:\n",
    "        # Infer the input shape from the model's first layer\n",
    "        input_shape = model.input_shape if hasattr(model, 'input_shape') else None\n",
    "        if input_shape:\n",
    "            model.build(input_shape)\n",
    "        else:\n",
    "            # Use dummy input if input_shape is unknown\n",
    "            dummy_input = np.random.random((1, *model.layers[0].input_shape[1:]))\n",
    "            model(dummy_input)\n",
    "    \n",
    "    layer_configs = []  # List to store layer-wise configurations\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer_name = layer.name\n",
    "\n",
    "        # Construct layer configuration\n",
    "        layer_config = {\n",
    "            \"layer_name\": layer_name,\n",
    "            \"type\": layer.__class__.__name__,  # Correct way to get the class name\n",
    "            \"input_file_path\": f\"{save_dir}/input/{layer_name}_input.bin\",\n",
    "            \"output_file_path\": f\"{save_dir}/output/{layer_name}_output.bin\",\n",
    "            \"weights_file_paths\": [],  # To be filled if the layer has weights\n",
    "            \"attributes\": {\n",
    "                \"input_shape\": list(layer.input_shape) if hasattr(layer, 'input_shape') else \"Unknown\",\n",
    "                \"output_shape\": list(layer.output_shape) if hasattr(layer, 'output_shape') else \"Unknown\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Add weights and biases if applicable\n",
    "        if hasattr(layer, 'weights') and layer.weights:\n",
    "            for weight in layer.weights:\n",
    "                weight_name = weight.name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "                weight_file_path = f\"{save_dir}/weights/{layer_name}_{weight_name}.bin\"\n",
    "                layer_config[\"weights_file_paths\"].append(weight_file_path)\n",
    "\n",
    "        # Add other layer-specific attributes if available\n",
    "        if hasattr(layer, 'kernel_size'):\n",
    "            layer_config[\"attributes\"][\"kernel_size\"] = layer.kernel_size\n",
    "        if hasattr(layer, 'strides'):\n",
    "            layer_config[\"attributes\"][\"strides\"] = layer.strides\n",
    "        if hasattr(layer, 'padding'):\n",
    "            layer_config[\"attributes\"][\"padding\"] = layer.padding\n",
    "        if hasattr(layer, 'activation') and layer.activation:\n",
    "            layer_config[\"attributes\"][\"activation\"] = layer.activation.__name__\n",
    "\n",
    "        # Append this layer's config to the list\n",
    "        layer_configs.append(layer_config)\n",
    "\n",
    "    # Save all configurations to a JSON file\n",
    "    with open(config_file_path, 'w') as config_file:\n",
    "        json.dump({\"layers\": layer_configs}, config_file, indent=4)\n",
    "\n",
    "    print(f\"Configuration file saved to {config_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2392f54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file saved to C:/Users/HP/Operators_Assignment/configs/json/model_config.json\n"
     ]
    }
   ],
   "source": [
    "# Assuming the `data` folder is in the current directory\n",
    "data_folder = \"C:/Users/HP/Operators_Assignment/data\"\n",
    "output_json_file = \"C:/Users/HP/Operators_Assignment/configs/json/model_config.json\"\n",
    "model = tf.keras.models.load_model(\"C:/Users/HP/Operators_Assignment/cnn_cifar10_model.h5\")\n",
    "\n",
    "generate_layer_config(model, data_folder, output_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc1ba7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               262272    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 356810 (1.36 MB)\n",
      "Trainable params: 356810 (1.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d66f0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.layers[0].input_shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83cfe17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "config_file_path = \"C:/Users/HP/Operators_Assignment/configs/json/model_config.json\"\n",
    "with open(config_file_path, \"r\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "# Access layers array from config\n",
    "layers = config[\"layers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b40aaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d\n",
      "max_pooling2d\n",
      "conv2d_1\n",
      "max_pooling2d_1\n",
      "conv2d_2\n",
      "flatten\n",
      "dense\n",
      "dense_1\n"
     ]
    }
   ],
   "source": [
    "for layer in layers:\n",
    "    print(layer['layer_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c567ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
