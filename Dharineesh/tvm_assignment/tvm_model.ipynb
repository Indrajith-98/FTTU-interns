{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "import onnx\n",
    "from tvm.contrib import graph_executor\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the ONNX model\n",
    "onnx_model = onnx.load(r'/home/dharineesh22/tvm_assignment/base_resnet_model.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the input shape for the model\n",
    "shape_dict = {\"input\": (1, 32, 32, 1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Convert the ONNX model to TVM relay format\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Optimize the model using TVM (ensure optimization passes)\n",
    "target = \"llvm\"\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    mod = relay.transform.InferType()(mod)  # Ensure types are inferred\n",
    "    optimized_mod = relay.transform.EliminateCommonSubexpr()(mod)  # Apply optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "# 5. Compile the optimized model into a shared library (.so)\n",
    "lib = relay.build(optimized_mod, target=target, params=params)\n",
    "\n",
    "# Save the compiled model as .so file\n",
    "lib.export_library('optimized_model.so')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Run inference on the optimized model\n",
    "# Create the TVM runtime and set the input\n",
    "ctx = tvm.device(target,0)  # Choose the appropriate context (e.g., tvm.cuda(0) for GPU)\n",
    "# ctx = tvm.cpu()\n",
    "runtime = graph_executor.GraphModule(lib['default'](ctx))  # Use graph_executor instead of graph_runtime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference output: [[6.6519638e-42 1.4720455e-17 2.4196034e-27 1.0000000e+00 7.3476063e-20\n",
      "  9.9715603e-13 1.9307948e-31 5.2554139e-17 2.9285737e-41 5.3557067e-13]]\n"
     ]
    }
   ],
   "source": [
    "# Create random input data (adjust size to match your model's input shape)\n",
    "input_data = np.random.uniform(-1, 1, size=(1, 32, 32, 1)).astype(\"float32\")\n",
    "\n",
    "# Set the input to the model\n",
    "runtime.set_input(\"input\", input_data)\n",
    "\n",
    "# Run inference\n",
    "runtime.run()\n",
    "\n",
    "# Get the output\n",
    "output = runtime.get_output(0).asnumpy()\n",
    "\n",
    "print(\"Inference output:\", output)\n",
    "\n",
    "# cpu inference output:\n",
    "\n",
    "# Inference output: [[2.7283281e-42 1.4005959e-17 2.0017176e-27 1.0000000e+00 1.7657622e-19\n",
    "#   6.9533849e-14 6.1296901e-31 2.6723934e-16 8.2944257e-41 3.0368763e-13]]\n",
    "\n",
    "# gpu inference output:\n",
    "\n",
    "# Inference output: [[6.6519638e-42 1.4720455e-17 2.4196034e-27 1.0000000e+00 7.3476063e-20\n",
    "#   9.9715603e-13 1.9307948e-31 5.2554139e-17 2.9285737e-41 5.3557067e-13]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_json = lib.get_graph_json()  # Get the graph as JSON\n",
    "with open('optimized_model_graph.json', 'w') as f:\n",
    "    f.write(graph_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
