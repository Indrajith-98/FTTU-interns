{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9CZnoFIyOZOf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "def load_and_preprocess_data():\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize the images to [0, 1] range\n",
        "\n",
        "    # Reshape the data to include the channel dimension (28, 28, 1)\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "metadata": {
        "id": "_5wue4caOcr-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data augmentation generator\n",
        "def create_data_augmentation_generator(x_train, y_train):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,           # Randomly rotate images by up to 10 degrees\n",
        "        width_shift_range=0.1,       # Randomly shift images horizontally by up to 10% of the width\n",
        "        height_shift_range=0.1,      # Randomly shift images vertically by up to 10% of the height\n",
        "        zoom_range=0.1,              # Randomly zoom into images by up to 10%\n",
        "        shear_range=0.1,             # Randomly shear images\n",
        "        fill_mode='nearest'          # Fill strategy for new pixels\n",
        "    )\n",
        "\n",
        "    # Fit the generator to the training data\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    return datagen"
      ],
      "metadata": {
        "id": "ciUMyf1jOwzZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = load_model('/content/drive/MyDrive/ResNet/best_resnet_mnist_model.h5')"
      ],
      "metadata": {
        "id": "KM2YuPx_O5eK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c20446-3dc4-4fe8-9c95-d2da7bd0b767"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to run the training with data augmentation\n",
        "if __name__ == '__main__':\n",
        "    (x_train, y_train), (x_test, y_test) = load_and_preprocess_data()\n",
        "\n",
        "    # Create the data augmentation generator\n",
        "    datagen = create_data_augmentation_generator(x_train, y_train)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model using the augmented data\n",
        "    model.fit(datagen.flow(x_train, y_train, batch_size=128),\n",
        "              epochs=10,\n",
        "              validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "6Q3W8BdOO6k7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50f0a4d-8338-4e32-9e90-c6f8424850c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 89ms/step - accuracy: 0.9901 - loss: 0.0311 - val_accuracy: 0.9933 - val_loss: 0.0230\n",
            "Epoch 2/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 74ms/step - accuracy: 0.9937 - loss: 0.0205 - val_accuracy: 0.9937 - val_loss: 0.0210\n",
            "Epoch 3/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 74ms/step - accuracy: 0.9954 - loss: 0.0146 - val_accuracy: 0.9941 - val_loss: 0.0201\n",
            "Epoch 4/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 70ms/step - accuracy: 0.9957 - loss: 0.0129 - val_accuracy: 0.9935 - val_loss: 0.0217\n",
            "Epoch 5/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 71ms/step - accuracy: 0.9957 - loss: 0.0130 - val_accuracy: 0.9941 - val_loss: 0.0195\n",
            "Epoch 6/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 74ms/step - accuracy: 0.9961 - loss: 0.0114 - val_accuracy: 0.9948 - val_loss: 0.0183\n",
            "Epoch 7/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 71ms/step - accuracy: 0.9962 - loss: 0.0114 - val_accuracy: 0.9942 - val_loss: 0.0188\n",
            "Epoch 8/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 76ms/step - accuracy: 0.9966 - loss: 0.0108 - val_accuracy: 0.9941 - val_loss: 0.0189\n",
            "Epoch 9/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 71ms/step - accuracy: 0.9966 - loss: 0.0106 - val_accuracy: 0.9947 - val_loss: 0.0181\n",
            "Epoch 10/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 72ms/step - accuracy: 0.9968 - loss: 0.0098 - val_accuracy: 0.9941 - val_loss: 0.0195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Save the augmented model\n",
        "    model.save('augmented_resnet_mnist_model.h5')\n",
        "\n",
        "    # Evaluate the augmented model and print the final accuracy\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
        "    print(f\"Final test accuracy with data augmentation: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "fgAZekscPBgX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9b6f41-e4ca-4bc1-bf12-58e06e0de52f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 2s - 5ms/step - accuracy: 0.9941 - loss: 0.0195\n",
            "Final test accuracy with data augmentation: 0.9941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vG5YZK6GPI3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5816ab32-729c-44db-897f-af09150052bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}