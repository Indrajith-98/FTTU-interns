{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TensorRT engine...\n",
      "[01/28/2025-17:47:14] [TRT] [E] IRuntime::deserializeCudaEngine: Error Code 1: Serialization (Serialization assertion plan->header.pad == expectedPlatformTag failed.Platform specific tag mismatch detected. TensorRT plan files are only supported on the target runtime platform they were created on.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to deserialize TensorRT engine.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     engine \u001b[38;5;241m=\u001b[39m runtime\u001b[38;5;241m.\u001b[39mdeserialize_cuda_engine(engine_data)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to deserialize TensorRT engine.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorRT engine loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(engine_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f, trt\u001b[38;5;241m.\u001b[39mRuntime(TRT_LOGGER) \u001b[38;5;28;01mas\u001b[39;00m runtime:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to deserialize TensorRT engine."
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load TensorRT Engine\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "engine_file = \"lenet_model.trt\"\n",
    "\n",
    "with open(engine_file, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    print(\"Loading TensorRT engine...\")\n",
    "    engine_data = f.read()\n",
    "    engine = runtime.deserialize_cuda_engine(engine_data)\n",
    "    if engine is None:\n",
    "        raise RuntimeError(\"Failed to deserialize TensorRT engine.\")\n",
    "    print(\"TensorRT engine loaded successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "with open(engine_file, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "# Allocate Buffers\n",
    "context = engine.create_execution_context()\n",
    "input_shape = engine.get_binding_shape(0)\n",
    "output_shape = engine.get_binding_shape(1)\n",
    "\n",
    "input_size = trt.volume(input_shape) * np.dtype(np.float32).itemsize\n",
    "output_size = trt.volume(output_shape) * np.dtype(np.float32).itemsize\n",
    "\n",
    "d_input = cuda.mem_alloc(input_size)\n",
    "d_output = cuda.mem_alloc(output_size)\n",
    "stream = cuda.Stream()\n",
    "\n",
    "# Generate a Dummy Input\n",
    "dummy_input_np = np.random.random((1, 1, 28, 28)).astype(np.float32)\n",
    "\n",
    "# Transfer Input to GPU\n",
    "cuda.memcpy_htod_async(d_input, dummy_input, stream)\n",
    "\n",
    "# Measure Latency\n",
    "num_iterations = 100\n",
    "latency_times = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    start_time = time.time()\n",
    "    context.execute_async_v2([int(d_input), int(d_output)], stream.handle)\n",
    "    stream.synchronize()\n",
    "    end_time = time.time()\n",
    "    latency_times.append(end_time - start_time)\n",
    "\n",
    "# Calculate Average Latency\n",
    "average_latency = sum(latency_times) / num_iterations\n",
    "print(f\"TensorRT Optimized Latency per Sample: {average_latency * 1000:.4f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
