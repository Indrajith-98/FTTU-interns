{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original IR Module:\n",
      "def @main(%input: Tensor[(1, 3, 32, 32), float32], %fc.weight: Tensor[(1000, 512), float32], %fc.bias: Tensor[(1000), float32], %onnx::Conv_193: Tensor[(64, 3, 7, 7), float32], %onnx::Conv_194: Tensor[(64), float32], %onnx::Conv_196: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_197: Tensor[(64), float32], %onnx::Conv_199: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_200: Tensor[(64), float32], %onnx::Conv_202: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_203: Tensor[(64), float32], %onnx::Conv_205: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_206: Tensor[(64), float32], %onnx::Conv_208: Tensor[(128, 64, 3, 3), float32], %onnx::Conv_209: Tensor[(128), float32], %onnx::Conv_211: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_212: Tensor[(128), float32], %onnx::Conv_214: Tensor[(128, 64, 1, 1), float32], %onnx::Conv_215: Tensor[(128), float32], %onnx::Conv_217: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_218: Tensor[(128), float32], %onnx::Conv_220: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_221: Tensor[(128), float32], %onnx::Conv_223: Tensor[(256, 128, 3, 3), float32], %onnx::Conv_224: Tensor[(256), float32], %onnx::Conv_226: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_227: Tensor[(256), float32], %onnx::Conv_229: Tensor[(256, 128, 1, 1), float32], %onnx::Conv_230: Tensor[(256), float32], %onnx::Conv_232: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_233: Tensor[(256), float32], %onnx::Conv_235: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_236: Tensor[(256), float32], %onnx::Conv_238: Tensor[(512, 256, 3, 3), float32], %onnx::Conv_239: Tensor[(512), float32], %onnx::Conv_241: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_242: Tensor[(512), float32], %onnx::Conv_244: Tensor[(512, 256, 1, 1), float32], %onnx::Conv_245: Tensor[(512), float32], %onnx::Conv_247: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_248: Tensor[(512), float32], %onnx::Conv_250: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_251: Tensor[(512), float32]) {\n",
      "  %0 = nn.conv2d(%input, %onnx::Conv_193, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]);\n",
      "  %1 = nn.bias_add(%0, %onnx::Conv_194);\n",
      "  %2 = nn.relu(%1);\n",
      "  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]);\n",
      "  %4 = nn.conv2d(%3, %onnx::Conv_196, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %5 = nn.bias_add(%4, %onnx::Conv_197);\n",
      "  %6 = nn.relu(%5);\n",
      "  %7 = nn.conv2d(%6, %onnx::Conv_199, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %8 = nn.bias_add(%7, %onnx::Conv_200);\n",
      "  %9 = add(%8, %3);\n",
      "  %10 = nn.relu(%9);\n",
      "  %11 = nn.conv2d(%10, %onnx::Conv_202, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %12 = nn.bias_add(%11, %onnx::Conv_203);\n",
      "  %13 = nn.relu(%12);\n",
      "  %14 = nn.conv2d(%13, %onnx::Conv_205, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %15 = nn.bias_add(%14, %onnx::Conv_206);\n",
      "  %16 = add(%15, %10);\n",
      "  %17 = nn.relu(%16);\n",
      "  %18 = nn.conv2d(%17, %onnx::Conv_208, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %19 = nn.bias_add(%18, %onnx::Conv_209);\n",
      "  %20 = nn.relu(%19);\n",
      "  %21 = nn.conv2d(%20, %onnx::Conv_211, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %22 = nn.conv2d(%17, %onnx::Conv_214, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]);\n",
      "  %23 = nn.bias_add(%21, %onnx::Conv_212);\n",
      "  %24 = nn.bias_add(%22, %onnx::Conv_215);\n",
      "  %25 = add(%23, %24);\n",
      "  %26 = nn.relu(%25);\n",
      "  %27 = nn.conv2d(%26, %onnx::Conv_217, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %28 = nn.bias_add(%27, %onnx::Conv_218);\n",
      "  %29 = nn.relu(%28);\n",
      "  %30 = nn.conv2d(%29, %onnx::Conv_220, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %31 = nn.bias_add(%30, %onnx::Conv_221);\n",
      "  %32 = add(%31, %26);\n",
      "  %33 = nn.relu(%32);\n",
      "  %34 = nn.conv2d(%33, %onnx::Conv_223, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %35 = nn.bias_add(%34, %onnx::Conv_224);\n",
      "  %36 = nn.relu(%35);\n",
      "  %37 = nn.conv2d(%36, %onnx::Conv_226, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %38 = nn.conv2d(%33, %onnx::Conv_229, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]);\n",
      "  %39 = nn.bias_add(%37, %onnx::Conv_227);\n",
      "  %40 = nn.bias_add(%38, %onnx::Conv_230);\n",
      "  %41 = add(%39, %40);\n",
      "  %42 = nn.relu(%41);\n",
      "  %43 = nn.conv2d(%42, %onnx::Conv_232, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %44 = nn.bias_add(%43, %onnx::Conv_233);\n",
      "  %45 = nn.relu(%44);\n",
      "  %46 = nn.conv2d(%45, %onnx::Conv_235, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %47 = nn.bias_add(%46, %onnx::Conv_236);\n",
      "  %48 = add(%47, %42);\n",
      "  %49 = nn.relu(%48);\n",
      "  %50 = nn.conv2d(%49, %onnx::Conv_238, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
      "  %51 = nn.bias_add(%50, %onnx::Conv_239);\n",
      "  %52 = nn.relu(%51);\n",
      "  %53 = nn.conv2d(%52, %onnx::Conv_241, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
      "  %54 = nn.conv2d(%49, %onnx::Conv_244, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]);\n",
      "  %55 = nn.bias_add(%53, %onnx::Conv_242);\n",
      "  %56 = nn.bias_add(%54, %onnx::Conv_245);\n",
      "  %57 = add(%55, %56);\n",
      "  %58 = nn.relu(%57);\n",
      "  %59 = nn.conv2d(%58, %onnx::Conv_247, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
      "  %60 = nn.bias_add(%59, %onnx::Conv_248);\n",
      "  %61 = nn.relu(%60);\n",
      "  %62 = nn.conv2d(%61, %onnx::Conv_250, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
      "  %63 = nn.bias_add(%62, %onnx::Conv_251);\n",
      "  %64 = add(%63, %58);\n",
      "  %65 = nn.relu(%64);\n",
      "  %66 = nn.global_avg_pool2d(%65);\n",
      "  %67 = nn.batch_flatten(%66);\n",
      "  %68 = nn.batch_flatten(%67);\n",
      "  %69 = nn.dense(%68, %fc.weight, units=1000);\n",
      "  %70 = multiply(1f, %fc.bias);\n",
      "  add(%69, %70)\n",
      "}\n",
      "\n",
      "\n",
      "Updated IR Module after transformations:\n",
      "def @main(%input: Tensor[(1, 3, 32, 32), float32], %fc.weight: Tensor[(1000, 512), float32], %fc.bias: Tensor[(1000), float32], %onnx::Conv_193: Tensor[(64, 3, 7, 7), float32], %onnx::Conv_194: Tensor[(64), float32], %onnx::Conv_196: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_197: Tensor[(64), float32], %onnx::Conv_199: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_200: Tensor[(64), float32], %onnx::Conv_202: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_203: Tensor[(64), float32], %onnx::Conv_205: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_206: Tensor[(64), float32], %onnx::Conv_208: Tensor[(128, 64, 3, 3), float32], %onnx::Conv_209: Tensor[(128), float32], %onnx::Conv_211: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_212: Tensor[(128), float32], %onnx::Conv_214: Tensor[(128, 64, 1, 1), float32], %onnx::Conv_215: Tensor[(128), float32], %onnx::Conv_217: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_218: Tensor[(128), float32], %onnx::Conv_220: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_221: Tensor[(128), float32], %onnx::Conv_223: Tensor[(256, 128, 3, 3), float32], %onnx::Conv_224: Tensor[(256), float32], %onnx::Conv_226: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_227: Tensor[(256), float32], %onnx::Conv_229: Tensor[(256, 128, 1, 1), float32], %onnx::Conv_230: Tensor[(256), float32], %onnx::Conv_232: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_233: Tensor[(256), float32], %onnx::Conv_235: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_236: Tensor[(256), float32], %onnx::Conv_238: Tensor[(512, 256, 3, 3), float32], %onnx::Conv_239: Tensor[(512), float32], %onnx::Conv_241: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_242: Tensor[(512), float32], %onnx::Conv_244: Tensor[(512, 256, 1, 1), float32], %onnx::Conv_245: Tensor[(512), float32], %onnx::Conv_247: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_248: Tensor[(512), float32], %onnx::Conv_250: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_251: Tensor[(512), float32]) -> Tensor[(1, 1000), float32] {\n",
      "  %46 = fn (%p020: Tensor[(1, 3, 32, 32), float32], %p117: Tensor[(64, 3, 7, 7), float32], %p217: Tensor[(64), float32], Primitive=1) -> Tensor[(1, 64, 16, 16), float32] {\n",
      "    %44 = nn.conv2d(%p020, %p117, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 16, 16), float32] */;\n",
      "    %45 = nn.bias_add(%44, %p217) /* ty=Tensor[(1, 64, 16, 16), float32] */;\n",
      "    nn.relu(%45) /* ty=Tensor[(1, 64, 16, 16), float32] */\n",
      "  };\n",
      "  %47 = %46(%input, %onnx::Conv_193, %onnx::Conv_194) /* ty=Tensor[(1, 64, 16, 16), float32] */;\n",
      "  %48 = fn (%p019: Tensor[(1, 64, 16, 16), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    nn.max_pool2d(%p019, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %49 = %48(%47) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %50 = fn (%p018: Tensor[(1, 64, 8, 8), float32], %p116: Tensor[(64, 64, 3, 3), float32], %p216: Tensor[(64), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %42 = nn.conv2d(%p018, %p116, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %43 = nn.bias_add(%42, %p216) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%43) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %51 = %50(%49, %onnx::Conv_196, %onnx::Conv_197) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %52 = fn (%p017: Tensor[(1, 64, 8, 8), float32], %p115: Tensor[(64, 64, 3, 3), float32], %p215: Tensor[(64), float32], %p37: Tensor[(1, 64, 8, 8), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %39 = nn.conv2d(%p017, %p115, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %40 = nn.bias_add(%39, %p215) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %41 = add(%40, %p37) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%41) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %53 = %52(%51, %onnx::Conv_199, %onnx::Conv_200, %49) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %54 = fn (%p016: Tensor[(1, 64, 8, 8), float32], %p114: Tensor[(64, 64, 3, 3), float32], %p214: Tensor[(64), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %37 = nn.conv2d(%p016, %p114, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %38 = nn.bias_add(%37, %p214) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%38) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %55 = %54(%53, %onnx::Conv_202, %onnx::Conv_203) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %56 = fn (%p015: Tensor[(1, 64, 8, 8), float32], %p113: Tensor[(64, 64, 3, 3), float32], %p213: Tensor[(64), float32], %p36: Tensor[(1, 64, 8, 8), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %34 = nn.conv2d(%p015, %p113, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %35 = nn.bias_add(%34, %p213) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %36 = add(%35, %p36) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%36) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %57 = %56(%55, %onnx::Conv_205, %onnx::Conv_206, %53) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %58 = fn (%p014: Tensor[(1, 64, 8, 8), float32], %p112: Tensor[(128, 64, 3, 3), float32], %p212: Tensor[(128), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %32 = nn.conv2d(%p014, %p112, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %33 = nn.bias_add(%32, %p212) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%33) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %60 = fn (%p021: Tensor[(1, 64, 8, 8), float32], %p118: Tensor[(128, 64, 1, 1), float32], %p218: Tensor[(128), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %59 = nn.conv2d(%p021, %p118, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.bias_add(%59, %p218) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %61 = %58(%57, %onnx::Conv_208, %onnx::Conv_209) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %62 = %60(%57, %onnx::Conv_214, %onnx::Conv_215) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %63 = fn (%p013: Tensor[(1, 128, 4, 4), float32], %p111: Tensor[(128, 128, 3, 3), float32], %p211: Tensor[(128), float32], %p35: Tensor[(1, 128, 4, 4), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %29 = nn.conv2d(%p013, %p111, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %30 = nn.bias_add(%29, %p211) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %31 = add(%30, %p35) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%31) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %64 = %63(%61, %onnx::Conv_211, %onnx::Conv_212, %62) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %65 = fn (%p012: Tensor[(1, 128, 4, 4), float32], %p110: Tensor[(128, 128, 3, 3), float32], %p210: Tensor[(128), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %27 = nn.conv2d(%p012, %p110, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %28 = nn.bias_add(%27, %p210) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%28) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %66 = %65(%64, %onnx::Conv_217, %onnx::Conv_218) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %67 = fn (%p011: Tensor[(1, 128, 4, 4), float32], %p19: Tensor[(128, 128, 3, 3), float32], %p29: Tensor[(128), float32], %p34: Tensor[(1, 128, 4, 4), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %24 = nn.conv2d(%p011, %p19, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %25 = nn.bias_add(%24, %p29) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %26 = add(%25, %p34) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%26) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %68 = %67(%66, %onnx::Conv_220, %onnx::Conv_221, %64) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %69 = fn (%p010: Tensor[(1, 128, 4, 4), float32], %p18: Tensor[(256, 128, 3, 3), float32], %p28: Tensor[(256), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %22 = nn.conv2d(%p010, %p18, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %23 = nn.bias_add(%22, %p28) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%23) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %71 = fn (%p022: Tensor[(1, 128, 4, 4), float32], %p119: Tensor[(256, 128, 1, 1), float32], %p219: Tensor[(256), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %70 = nn.conv2d(%p022, %p119, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.bias_add(%70, %p219) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %72 = %69(%68, %onnx::Conv_223, %onnx::Conv_224) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %73 = %71(%68, %onnx::Conv_229, %onnx::Conv_230) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %74 = fn (%p09: Tensor[(1, 256, 2, 2), float32], %p17: Tensor[(256, 256, 3, 3), float32], %p27: Tensor[(256), float32], %p33: Tensor[(1, 256, 2, 2), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %19 = nn.conv2d(%p09, %p17, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %20 = nn.bias_add(%19, %p27) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %21 = add(%20, %p33) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%21) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %75 = %74(%72, %onnx::Conv_226, %onnx::Conv_227, %73) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %76 = fn (%p08: Tensor[(1, 256, 2, 2), float32], %p16: Tensor[(256, 256, 3, 3), float32], %p26: Tensor[(256), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %17 = nn.conv2d(%p08, %p16, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %18 = nn.bias_add(%17, %p26) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%18) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %77 = %76(%75, %onnx::Conv_232, %onnx::Conv_233) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %78 = fn (%p07: Tensor[(1, 256, 2, 2), float32], %p15: Tensor[(256, 256, 3, 3), float32], %p25: Tensor[(256), float32], %p32: Tensor[(1, 256, 2, 2), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %14 = nn.conv2d(%p07, %p15, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %15 = nn.bias_add(%14, %p25) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %16 = add(%15, %p32) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%16) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %79 = %78(%77, %onnx::Conv_235, %onnx::Conv_236, %75) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %80 = fn (%p06: Tensor[(1, 256, 2, 2), float32], %p14: Tensor[(512, 256, 3, 3), float32], %p24: Tensor[(512), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %12 = nn.conv2d(%p06, %p14, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %13 = nn.bias_add(%12, %p24) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%13) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %82 = fn (%p023: Tensor[(1, 256, 2, 2), float32], %p120: Tensor[(512, 256, 1, 1), float32], %p220: Tensor[(512), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %81 = nn.conv2d(%p023, %p120, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.bias_add(%81, %p220) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %83 = %80(%79, %onnx::Conv_238, %onnx::Conv_239) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %84 = %82(%79, %onnx::Conv_244, %onnx::Conv_245) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %85 = fn (%p05: Tensor[(1, 512, 1, 1), float32], %p13: Tensor[(512, 512, 3, 3), float32], %p23: Tensor[(512), float32], %p31: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %9 = nn.conv2d(%p05, %p13, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %10 = nn.bias_add(%9, %p23) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %11 = add(%10, %p31) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%11) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %86 = %85(%83, %onnx::Conv_241, %onnx::Conv_242, %84) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %87 = fn (%p04: Tensor[(1, 512, 1, 1), float32], %p12: Tensor[(512, 512, 3, 3), float32], %p22: Tensor[(512), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %7 = nn.conv2d(%p04, %p12, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %8 = nn.bias_add(%7, %p22) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%8) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %88 = %87(%86, %onnx::Conv_247, %onnx::Conv_248) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %89 = fn (%p03: Tensor[(1, 512, 1, 1), float32], %p11: Tensor[(512, 512, 3, 3), float32], %p21: Tensor[(512), float32], %p3: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %4 = nn.conv2d(%p03, %p11, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %5 = nn.bias_add(%4, %p21) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %6 = add(%5, %p3) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%6) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %90 = %89(%88, %onnx::Conv_250, %onnx::Conv_251, %86) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %91 = fn (%p02: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    nn.global_avg_pool2d(%p02) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %92 = %91(%90) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %93 = fn (%p01: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512), float32] {\n",
      "    %3 = nn.batch_flatten(%p01) /* ty=Tensor[(1, 512), float32] */;\n",
      "    nn.batch_flatten(%3) /* ty=Tensor[(1, 512), float32] */\n",
      "  };\n",
      "  %94 = %93(%92) /* ty=Tensor[(1, 512), float32] */;\n",
      "  %95 = fn (%p0: Tensor[(1, 512), float32], %p1: Tensor[(1000, 512), float32], %p2: Tensor[(1000), float32], Primitive=1) -> Tensor[(1, 1000), float32] {\n",
      "    %0 = multiply(1f /* ty=float32 */, %p2) /* ty=Tensor[(1000), float32] */;\n",
      "    %1 = nn.dense(%p0, %p1, units=1000) /* ty=Tensor[(1, 1000), float32] */;\n",
      "    %2 = expand_dims(%0, axis=0) /* ty=Tensor[(1, 1000), float32] */;\n",
      "    add(%1, %2) /* ty=Tensor[(1, 1000), float32] */\n",
      "  };\n",
      "  %95(%94, %fc.weight, %fc.bias) /* ty=Tensor[(1, 1000), float32] */\n",
      "}\n",
      "\n",
      "Updated IR Module saved to updated_ir_module.json\n",
      "Inference Output: [[  0.35554588  -2.7390285    8.705973     2.6447318    7.32295\n",
      "    5.0002623   12.574636     2.1368818   -2.506182    -0.16612542\n",
      "  -12.127576   -11.431837   -12.060638   -11.17637    -10.379433\n",
      "  -11.650353   -12.050523   -11.077483   -11.045261   -10.611672\n",
      "  -10.581473   -10.859575   -10.4894495  -12.237125   -11.380078\n",
      "  -11.729806   -12.570196   -12.425205   -10.975765   -13.434034\n",
      "  -10.644159   -12.41315    -11.919635    -9.299596    -8.529058\n",
      "  -11.232497   -10.351465   -10.433548   -11.271473   -10.591515\n",
      "  -11.011014   -10.345389   -11.350908   -10.762518   -11.787111\n",
      "  -10.896193   -10.635954   -11.956749   -10.707249   -10.526702\n",
      "  -10.167468    -9.939807   -10.991504   -11.031563   -10.620986\n",
      "   -9.894704   -10.334735    -9.731097   -11.077904   -10.788399\n",
      "  -10.922326   -10.303945   -10.002742   -10.911656   -10.4702015\n",
      "   -9.902748   -11.483096   -10.051438   -10.474192    -9.347344\n",
      "  -10.679393   -10.048754   -10.161867   -10.497142   -10.633089\n",
      "  -10.027097    -9.461314   -10.927763   -10.52042    -10.10323\n",
      "  -12.755398   -10.57295    -11.8650255  -12.003806   -11.070511\n",
      "   -9.70591    -12.697156   -11.306776   -11.131945    -9.965013\n",
      "  -12.687834   -13.12364    -10.005789   -12.443427   -10.378169\n",
      "  -11.334251   -10.987751    -9.621618   -11.252142   -10.914732\n",
      "  -11.513713   -10.514605   -10.326141   -10.922069   -10.268192\n",
      "   -9.845466    -9.594398    -8.459783   -10.440452   -10.014272\n",
      "   -9.783536   -10.6320305   -9.338267   -11.224591   -11.339723\n",
      "  -11.4589     -11.656081   -11.295299    -9.507706   -10.930322\n",
      "   -9.725965    -9.870189   -11.000062   -10.038431    -9.895914\n",
      "   -9.2558     -11.448157   -11.524569   -11.48978    -10.57435\n",
      "  -12.501197   -10.895928   -10.51538    -12.307266   -11.916421\n",
      "  -11.2632     -11.727361   -10.694505   -11.000402   -10.184393\n",
      "   -9.39403    -11.11949    -10.261406   -10.835865   -11.169733\n",
      "  -10.012614    -8.628669    -9.83163     -9.177277   -10.270051\n",
      "   -9.634508   -10.519874   -10.904742    -9.800654    -9.2807665\n",
      "   -9.214928   -10.395233   -11.032385   -10.403616   -11.225108\n",
      "  -11.4550085   -9.4465475   -9.042416   -10.642509   -10.949301\n",
      "  -11.044713   -10.066605   -11.184844    -9.665657    -9.895283\n",
      "  -10.357841    -8.347422    -9.5910225  -10.592355   -10.4767475\n",
      "  -11.431341   -11.455566   -11.484423    -9.307381   -10.0529\n",
      "   -9.52523    -10.287831   -10.090331   -10.806812    -9.413487\n",
      "  -10.386527   -10.699873   -10.858903   -10.237507    -9.766676\n",
      "  -10.12627    -10.488164   -10.21147    -10.479159   -11.409927\n",
      "   -9.442313    -9.848943   -11.188011   -10.508936   -10.590001\n",
      "  -11.258786    -9.767898   -10.372306    -9.235384    -9.416433\n",
      "  -11.611886   -11.461975   -10.334025   -10.9237995   -8.735996\n",
      "  -10.42261    -10.327552   -10.619567    -9.994408   -12.544613\n",
      "  -10.2817335  -11.275379    -9.917796   -10.457211   -10.8648\n",
      "  -11.232614   -10.649487   -11.535079    -9.964202   -11.6851425\n",
      "  -11.411854   -11.488356   -11.050491   -11.212466    -9.621163\n",
      "   -9.132296   -10.118443   -10.863959   -11.789744   -11.453292\n",
      "  -11.420002   -10.497398   -10.671667   -10.22487    -11.257951\n",
      "  -11.3640995  -11.001561    -9.308875   -11.062698   -12.350145\n",
      "   -9.699121   -10.683125   -10.377382   -10.305446   -10.388435\n",
      "   -9.981478    -9.621089   -11.721235   -10.294744    -9.947808\n",
      "  -13.059925   -11.023916    -9.245389    -9.417006   -10.260317\n",
      "  -10.097903    -9.890528   -12.411235    -9.265381    -9.959379\n",
      "  -10.240904   -10.517236   -10.724945   -10.798044   -10.942847\n",
      "  -10.722106   -11.826593   -10.889692   -10.457842   -13.116448\n",
      "  -12.162059   -11.131319   -12.391513   -12.674102   -10.754965\n",
      "  -11.803281   -10.366322   -10.570241   -10.1022      -9.938451\n",
      "  -11.157955   -10.526116   -10.97147    -11.259618   -11.247386\n",
      "  -11.356339   -11.225346   -11.075114   -10.998705   -10.479781\n",
      "  -10.567151    -9.975273   -11.908431   -11.769102   -11.135865\n",
      "  -11.065782   -12.872165   -11.136506   -10.656867   -11.236644\n",
      "   -9.857088   -10.442423   -11.873118   -11.287825   -11.47575\n",
      "  -11.514837   -11.196655   -11.598065   -10.331496    -9.630097\n",
      "  -10.670346   -11.5965395  -12.25164    -11.147123   -11.276941\n",
      "  -12.173223   -12.007344   -13.649363   -11.99707    -11.029065\n",
      "  -12.123637   -12.15219     -9.3470955   -9.994419    -9.873044\n",
      "  -11.148472   -11.828666   -10.541879    -9.965719   -10.786283\n",
      "  -12.263527   -11.022595   -10.551178   -10.79549    -10.410125\n",
      "  -11.572979   -10.593143   -10.339472    -9.718308    -9.47531\n",
      "   -9.676373   -10.234802   -11.043142   -11.694992   -11.76995\n",
      "  -11.252504   -12.64588    -11.565863   -11.850607   -11.030209\n",
      "  -11.774813   -10.176091   -11.608725   -10.728483   -10.307587\n",
      "   -9.861259   -10.084694   -10.135537   -10.308975   -11.232337\n",
      "  -10.188063   -11.040212   -11.285661   -12.294554   -11.825194\n",
      "  -10.983167   -12.279508   -10.749257   -10.66462    -11.188091\n",
      "  -11.175874   -12.064173   -12.118929   -11.8173485  -11.632062\n",
      "  -12.216982   -11.935065   -11.438839   -11.386563   -13.337424\n",
      "  -10.038093    -9.750375   -12.347232   -10.6551485  -11.347017\n",
      "  -10.716934    -9.551461   -11.397204   -10.9592905   -9.51881\n",
      "  -11.027009    -9.8875065   -8.737729   -10.104393   -10.734197\n",
      "  -10.640309   -10.265832    -9.205424    -9.920109   -10.508785\n",
      "  -10.878614   -11.297041   -11.118037   -10.5377865  -10.444539\n",
      "  -11.03545     -9.761414    -9.7305975  -10.340719    -9.811753\n",
      "  -10.25678    -11.136232   -10.367944   -11.239382    -8.662267\n",
      "  -10.389724   -11.264015    -9.24082    -10.423003   -10.456182\n",
      "  -11.113686    -9.910762   -11.364556   -10.485012   -10.537554\n",
      "   -9.844018   -11.230686    -9.323081    -9.33244    -10.020575\n",
      "  -10.424606   -11.458063    -9.858881   -10.035075   -12.227715\n",
      "   -9.543501    -8.754678    -9.564196    -9.008938   -11.047807\n",
      "   -8.341012   -10.91995     -9.35914    -10.140773   -11.177802\n",
      "  -10.994236    -8.802587    -9.933337   -10.285789   -10.267331\n",
      "   -9.670662    -9.679831    -9.251316   -10.3413515   -9.576847\n",
      "   -8.781375    -9.102254   -10.092147    -9.709655   -10.179214\n",
      "  -11.026336    -9.870466   -10.433699   -10.482451   -10.859361\n",
      "   -9.929796   -10.544439   -10.111342   -10.866615    -8.672623\n",
      "   -9.829087   -10.373476   -11.078779    -8.977197   -11.212706\n",
      "   -9.860047   -10.496764    -9.89323    -10.707722    -9.695172\n",
      "  -10.180386   -10.384165    -9.986132   -10.374708   -10.453795\n",
      "   -9.48261    -11.185488   -10.36479    -10.781981   -10.302194\n",
      "  -10.703591    -9.3771     -10.469466    -9.181181   -10.586781\n",
      "   -9.434706   -10.069311   -10.370461   -10.484067   -10.540868\n",
      "   -9.694464   -10.857881   -10.0393715   -9.870846   -10.942811\n",
      "  -10.42552    -10.438676   -10.918523    -9.027242    -9.999993\n",
      "   -9.518065   -11.01847    -11.504501   -10.358246   -11.265008\n",
      "  -11.428818   -10.055734   -11.044344    -8.299554    -9.551283\n",
      "  -10.726721   -11.238609   -10.896348   -10.288357    -9.363636\n",
      "  -10.538075   -10.077553   -10.085468    -9.17608    -11.417413\n",
      "  -11.773349   -10.308994   -10.667108   -10.2939005   -9.825403\n",
      "  -11.055833   -10.765475   -10.4284935  -10.134938   -10.391766\n",
      "   -9.534152   -11.100506   -11.374921   -10.771944    -9.898461\n",
      "  -10.863509   -10.684541    -9.571864   -10.048213   -10.255077\n",
      "  -10.979557   -11.052982   -10.062762   -10.455193    -9.860422\n",
      "   -9.617089   -10.298034    -9.563551   -10.921004   -11.263573\n",
      "  -10.394155    -8.707827   -10.685878   -10.379489   -11.135126\n",
      "  -10.592512   -10.639774   -10.556627   -10.334665   -10.3635\n",
      "  -10.339941   -10.307187    -9.652741   -10.071608   -10.556891\n",
      "  -11.953768   -10.515903   -11.214423   -10.49861    -10.387961\n",
      "   -9.631557   -11.872395    -9.774619    -9.97945     -9.61989\n",
      "  -11.859657    -9.69965    -11.6601715  -10.857813    -9.864691\n",
      "  -10.915095    -9.160924    -8.446747   -10.257939   -10.177876\n",
      "  -10.655556   -10.500767   -10.663252   -11.681305   -10.608079\n",
      "   -9.629973    -9.2138605  -10.573769    -9.627249   -11.707331\n",
      "   -8.645416    -9.370621   -10.116367    -9.861589   -11.1002445\n",
      "   -9.740472   -10.731381   -10.840057   -10.349007   -10.459681\n",
      "  -10.643529   -11.646272   -10.8410635  -11.142702   -10.946081\n",
      "  -10.07797    -10.275821   -10.4758835   -9.537529   -10.182358\n",
      "   -8.499163    -9.009138   -10.965583   -10.904324   -12.322733\n",
      "  -11.311604    -9.73039    -10.300993    -8.487572    -8.943406\n",
      "   -8.342965   -11.00742     -9.7716465   -9.848802   -10.313958\n",
      "  -11.155503   -10.598587   -10.551549    -9.979591   -10.4316435\n",
      "  -10.384406   -10.983834   -10.164452   -11.466818   -10.67079\n",
      "   -9.155466   -10.552444   -10.426447    -8.713689   -10.812604\n",
      "  -11.003988   -11.873278   -10.65014    -10.288286   -10.7534275\n",
      "  -11.212497   -11.577648    -9.436478   -10.4933195   -9.993621\n",
      "  -11.03897    -11.740016   -10.903684    -9.734809    -9.329277\n",
      "  -10.7100935  -11.011955   -10.603775    -9.51379     -9.074854\n",
      "  -10.740315   -10.588694   -10.049889   -10.7619095  -10.102045\n",
      "  -10.737547   -10.803697   -10.115791   -11.11061    -11.18951\n",
      "  -10.662786    -9.470583    -9.784996    -9.898828   -10.247333\n",
      "  -10.515895    -9.515941   -10.805465   -10.114203   -10.810295\n",
      "   -9.942309    -9.73785    -11.857528   -11.275192   -10.49554\n",
      "  -10.510202   -10.161122    -9.746472   -12.188186    -9.949806\n",
      "  -10.143831    -9.714604    -9.937345   -11.8005295   -9.404177\n",
      "  -11.234205   -11.09781    -11.351463    -9.90147     -9.193517\n",
      "  -10.079407    -9.782815    -9.382617    -9.7534275   -9.673313\n",
      "   -9.870881   -10.367598   -10.744957    -9.630197   -12.582002\n",
      "  -10.488013    -9.294359   -10.813508    -9.57756    -11.03386\n",
      "   -8.486685   -10.177157    -9.346809   -11.398636   -11.728117\n",
      "   -9.340172   -10.47363    -10.189019   -10.92319    -10.23826\n",
      "  -10.868117   -10.159475    -9.585187    -9.915425   -10.118311\n",
      "  -10.104814   -10.1632595   -9.473424   -11.5208025  -10.81725\n",
      "  -11.219862   -11.13916    -10.953896    -9.343148   -10.890565\n",
      "  -10.371903   -10.70134    -10.790774    -9.327857   -10.407131\n",
      "  -10.5656595  -11.57404    -10.312989    -9.910657   -10.285986\n",
      "   -9.174081   -10.5794935   -9.801094   -10.442233    -9.278773\n",
      "  -10.04763    -10.073632   -10.026446    -9.741386   -10.551761\n",
      "  -10.182889   -11.504249   -11.081681   -10.950025    -9.344069\n",
      "  -10.053185   -10.927373    -9.76843    -10.262325   -11.725361\n",
      "  -10.838591    -9.581487    -9.137939    -9.279458   -10.385333\n",
      "  -10.280491   -10.512499   -10.831404   -11.5170765  -10.224143\n",
      "   -9.862739    -8.1787815  -11.360257   -11.121173    -9.761148\n",
      "   -8.794664    -9.301837   -10.865255    -9.660838   -11.0783615\n",
      "  -12.302162   -10.14223    -10.661159   -10.054573    -9.646691\n",
      "  -10.8303      -9.294668   -11.38357    -11.395399   -11.2111845\n",
      "  -11.99718    -11.363337   -10.473155    -9.039758    -9.309965\n",
      "  -12.889549   -10.021449   -10.427646    -9.735173   -10.198959\n",
      "  -10.3615885  -10.993121   -10.380531    -9.096829    -9.375511\n",
      "  -10.331192    -9.042641    -8.905709    -8.861806    -9.767308\n",
      "  -10.245451    -9.063089    -8.787279    -9.603777   -10.559335\n",
      "   -8.873034   -10.694494   -11.065146   -10.574241   -10.03635\n",
      "  -10.3823805  -10.265389   -10.042806   -10.436466   -11.268576\n",
      "  -10.28314    -12.318387   -10.941552    -9.950429   -10.304151\n",
      "  -11.148061   -11.003318   -11.137382   -10.940584   -11.023174\n",
      "  -12.337945   -10.878446   -10.8176155  -10.555555   -10.058061\n",
      "   -9.849146   -10.121517   -10.356134    -9.696349   -11.613999\n",
      "   -9.177118   -10.253946    -9.795701   -12.5306835   -9.094001\n",
      "  -10.177604   -11.172718    -9.6850815  -10.919411   -10.568784\n",
      "  -10.990632   -10.173818   -10.564373   -11.775454    -9.7222185\n",
      "   -8.77639    -10.622708    -9.854012    -9.972899   -10.818198\n",
      "  -10.043057   -10.368722    -9.965984    -9.3445215   -9.837172\n",
      "  -10.366071    -9.557492   -10.264911    -9.573342   -11.664291\n",
      "  -11.138917    -9.818556    -9.789885    -9.028732   -10.27154\n",
      "  -10.207727    -9.9780445  -13.125202    -8.862188   -11.371295\n",
      "  -11.011598   -11.095414   -10.685169   -10.241072   -10.532415\n",
      "   -9.302524   -11.554648   -11.417717   -10.653468   -10.883387\n",
      "  -10.584386   -11.070466    -9.596824    -9.535458    -9.21218\n",
      "  -11.232019   -10.549366    -9.096963   -10.181479   -10.254887\n",
      "   -9.882129   -11.527344   -10.676135   -10.616629   -12.051657\n",
      "  -11.744515   -11.059507   -11.1909     -11.906762   -10.939401\n",
      "  -10.241779   -11.959509   -11.239196   -10.241442   -10.628161\n",
      "  -10.893503   -11.183906   -12.028624   -10.676068   -10.442733\n",
      "  -10.111643   -11.804461   -10.106857   -10.461463   -10.953614\n",
      "   -8.784288    -9.701005   -10.876234   -10.303861   -11.733729\n",
      "  -11.19254     -9.742202   -10.04758    -11.850762   -10.725695\n",
      "  -10.854559    -9.403596   -10.327686    -9.788956    -9.185324\n",
      "  -10.07962     -9.043324    -7.729777    -9.0357485  -10.73737\n",
      "   -9.750442   -11.490503    -9.56479     -8.755341   -10.378864\n",
      "  -11.70489    -10.785689   -10.725252   -11.586172   -11.957841\n",
      "  -11.651068   -12.1323805  -11.449952   -11.649508   -11.87021\n",
      "  -11.495658   -12.475047   -10.526857   -11.282509    -8.742309  ]]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.relay import transform\n",
    "from tvm.contrib import graph_executor\n",
    "\n",
    "onnx_model_path = \"model.onnx\"  \n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "shape_dict = {\"input\": [1, 3, 32, 32]}  \n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "print(\"Original IR Module:\")\n",
    "print(mod)\n",
    "\n",
    "mod = transform.InferType()(mod)\n",
    "mod = transform.SimplifyInference()(mod)\n",
    "\n",
    "mod = transform.FuseOps(fuse_opt_level=2)(mod)\n",
    "\n",
    "mod = transform.AlterOpLayout()(mod)\n",
    "\n",
    "print(\"\\nUpdated IR Module after transformations:\")\n",
    "print(mod)\n",
    "\n",
    "visualize_path = \"updated_ir_module.json\"\n",
    "with open(visualize_path, \"w\") as f:\n",
    "    f.write(mod.astext(show_meta_data=False))\n",
    "print(f\"Updated IR Module saved to {visualize_path}\")\n",
    "\n",
    "target = \"llvm\"  \n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "dev = tvm.device(target, 0)\n",
    "module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n",
    "import numpy as np\n",
    "input_data = np.random.rand(1, 3, 32, 32).astype(\"float32\")\n",
    "module.set_input(\"input\",input_data)  \n",
    "module.run()\n",
    "output = module.get_output(0).asnumpy()\n",
    "print(\"Inference Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.export_library(\"optimized_model.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_params.params\", \"wb\") as f:\n",
    "    f.write(relay.save_param_dict(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"optimized_model_structure.json\",\"w\") as f:\n",
    "    f.write(lib.get_graph_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
