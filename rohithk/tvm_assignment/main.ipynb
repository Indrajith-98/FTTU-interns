{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original IR Module:\n",
      "def @main(%input: Tensor[(1, 3, 32, 32), float32], %fc.weight: Tensor[(1000, 512), float32], %fc.bias: Tensor[(1000), float32], %onnx::Conv_193: Tensor[(64, 3, 7, 7), float32], %onnx::Conv_194: Tensor[(64), float32], %onnx::Conv_196: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_197: Tensor[(64), float32], %onnx::Conv_199: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_200: Tensor[(64), float32], %onnx::Conv_202: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_203: Tensor[(64), float32], %onnx::Conv_205: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_206: Tensor[(64), float32], %onnx::Conv_208: Tensor[(128, 64, 3, 3), float32], %onnx::Conv_209: Tensor[(128), float32], %onnx::Conv_211: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_212: Tensor[(128), float32], %onnx::Conv_214: Tensor[(128, 64, 1, 1), float32], %onnx::Conv_215: Tensor[(128), float32], %onnx::Conv_217: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_218: Tensor[(128), float32], %onnx::Conv_220: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_221: Tensor[(128), float32], %onnx::Conv_223: Tensor[(256, 128, 3, 3), float32], %onnx::Conv_224: Tensor[(256), float32], %onnx::Conv_226: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_227: Tensor[(256), float32], %onnx::Conv_229: Tensor[(256, 128, 1, 1), float32], %onnx::Conv_230: Tensor[(256), float32], %onnx::Conv_232: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_233: Tensor[(256), float32], %onnx::Conv_235: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_236: Tensor[(256), float32], %onnx::Conv_238: Tensor[(512, 256, 3, 3), float32], %onnx::Conv_239: Tensor[(512), float32], %onnx::Conv_241: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_242: Tensor[(512), float32], %onnx::Conv_244: Tensor[(512, 256, 1, 1), float32], %onnx::Conv_245: Tensor[(512), float32], %onnx::Conv_247: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_248: Tensor[(512), float32], %onnx::Conv_250: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_251: Tensor[(512), float32]) {\n",
      "  %0 = nn.conv2d(%input, %onnx::Conv_193, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]);\n",
      "  %1 = nn.bias_add(%0, %onnx::Conv_194);\n",
      "  %2 = nn.relu(%1);\n",
      "  %3 = nn.max_pool2d(%2, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]);\n",
      "  %4 = nn.conv2d(%3, %onnx::Conv_196, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %5 = nn.bias_add(%4, %onnx::Conv_197);\n",
      "  %6 = nn.relu(%5);\n",
      "  %7 = nn.conv2d(%6, %onnx::Conv_199, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %8 = nn.bias_add(%7, %onnx::Conv_200);\n",
      "  %9 = add(%8, %3);\n",
      "  %10 = nn.relu(%9);\n",
      "  %11 = nn.conv2d(%10, %onnx::Conv_202, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %12 = nn.bias_add(%11, %onnx::Conv_203);\n",
      "  %13 = nn.relu(%12);\n",
      "  %14 = nn.conv2d(%13, %onnx::Conv_205, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %15 = nn.bias_add(%14, %onnx::Conv_206);\n",
      "  %16 = add(%15, %10);\n",
      "  %17 = nn.relu(%16);\n",
      "  %18 = nn.conv2d(%17, %onnx::Conv_208, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %19 = nn.bias_add(%18, %onnx::Conv_209);\n",
      "  %20 = nn.relu(%19);\n",
      "  %21 = nn.conv2d(%20, %onnx::Conv_211, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %22 = nn.conv2d(%17, %onnx::Conv_214, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]);\n",
      "  %23 = nn.bias_add(%21, %onnx::Conv_212);\n",
      "  %24 = nn.bias_add(%22, %onnx::Conv_215);\n",
      "  %25 = add(%23, %24);\n",
      "  %26 = nn.relu(%25);\n",
      "  %27 = nn.conv2d(%26, %onnx::Conv_217, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %28 = nn.bias_add(%27, %onnx::Conv_218);\n",
      "  %29 = nn.relu(%28);\n",
      "  %30 = nn.conv2d(%29, %onnx::Conv_220, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %31 = nn.bias_add(%30, %onnx::Conv_221);\n",
      "  %32 = add(%31, %26);\n",
      "  %33 = nn.relu(%32);\n",
      "  %34 = nn.conv2d(%33, %onnx::Conv_223, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %35 = nn.bias_add(%34, %onnx::Conv_224);\n",
      "  %36 = nn.relu(%35);\n",
      "  %37 = nn.conv2d(%36, %onnx::Conv_226, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %38 = nn.conv2d(%33, %onnx::Conv_229, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]);\n",
      "  %39 = nn.bias_add(%37, %onnx::Conv_227);\n",
      "  %40 = nn.bias_add(%38, %onnx::Conv_230);\n",
      "  %41 = add(%39, %40);\n",
      "  %42 = nn.relu(%41);\n",
      "  %43 = nn.conv2d(%42, %onnx::Conv_232, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %44 = nn.bias_add(%43, %onnx::Conv_233);\n",
      "  %45 = nn.relu(%44);\n",
      "  %46 = nn.conv2d(%45, %onnx::Conv_235, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %47 = nn.bias_add(%46, %onnx::Conv_236);\n",
      "  %48 = add(%47, %42);\n",
      "  %49 = nn.relu(%48);\n",
      "  %50 = nn.conv2d(%49, %onnx::Conv_238, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
      "  %51 = nn.bias_add(%50, %onnx::Conv_239);\n",
      "  %52 = nn.relu(%51);\n",
      "  %53 = nn.conv2d(%52, %onnx::Conv_241, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
      "  %54 = nn.conv2d(%49, %onnx::Conv_244, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]);\n",
      "  %55 = nn.bias_add(%53, %onnx::Conv_242);\n",
      "  %56 = nn.bias_add(%54, %onnx::Conv_245);\n",
      "  %57 = add(%55, %56);\n",
      "  %58 = nn.relu(%57);\n",
      "  %59 = nn.conv2d(%58, %onnx::Conv_247, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
      "  %60 = nn.bias_add(%59, %onnx::Conv_248);\n",
      "  %61 = nn.relu(%60);\n",
      "  %62 = nn.conv2d(%61, %onnx::Conv_250, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
      "  %63 = nn.bias_add(%62, %onnx::Conv_251);\n",
      "  %64 = add(%63, %58);\n",
      "  %65 = nn.relu(%64);\n",
      "  %66 = nn.global_avg_pool2d(%65);\n",
      "  %67 = nn.batch_flatten(%66);\n",
      "  %68 = nn.batch_flatten(%67);\n",
      "  %69 = nn.dense(%68, %fc.weight, units=1000);\n",
      "  %70 = multiply(1f, %fc.bias);\n",
      "  add(%69, %70)\n",
      "}\n",
      "\n",
      "\n",
      "Updated IR Module after transformations:\n",
      "def @main(%input: Tensor[(1, 3, 32, 32), float32], %fc.weight: Tensor[(1000, 512), float32], %fc.bias: Tensor[(1000), float32], %onnx::Conv_193: Tensor[(64, 3, 7, 7), float32], %onnx::Conv_194: Tensor[(64), float32], %onnx::Conv_196: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_197: Tensor[(64), float32], %onnx::Conv_199: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_200: Tensor[(64), float32], %onnx::Conv_202: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_203: Tensor[(64), float32], %onnx::Conv_205: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_206: Tensor[(64), float32], %onnx::Conv_208: Tensor[(128, 64, 3, 3), float32], %onnx::Conv_209: Tensor[(128), float32], %onnx::Conv_211: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_212: Tensor[(128), float32], %onnx::Conv_214: Tensor[(128, 64, 1, 1), float32], %onnx::Conv_215: Tensor[(128), float32], %onnx::Conv_217: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_218: Tensor[(128), float32], %onnx::Conv_220: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_221: Tensor[(128), float32], %onnx::Conv_223: Tensor[(256, 128, 3, 3), float32], %onnx::Conv_224: Tensor[(256), float32], %onnx::Conv_226: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_227: Tensor[(256), float32], %onnx::Conv_229: Tensor[(256, 128, 1, 1), float32], %onnx::Conv_230: Tensor[(256), float32], %onnx::Conv_232: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_233: Tensor[(256), float32], %onnx::Conv_235: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_236: Tensor[(256), float32], %onnx::Conv_238: Tensor[(512, 256, 3, 3), float32], %onnx::Conv_239: Tensor[(512), float32], %onnx::Conv_241: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_242: Tensor[(512), float32], %onnx::Conv_244: Tensor[(512, 256, 1, 1), float32], %onnx::Conv_245: Tensor[(512), float32], %onnx::Conv_247: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_248: Tensor[(512), float32], %onnx::Conv_250: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_251: Tensor[(512), float32]) -> Tensor[(1, 1000), float32] {\n",
      "  %46 = fn (%p020: Tensor[(1, 3, 32, 32), float32], %p117: Tensor[(64, 3, 7, 7), float32], %p217: Tensor[(64), float32], Primitive=1) -> Tensor[(1, 64, 16, 16), float32] {\n",
      "    %44 = nn.conv2d(%p020, %p117, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 16, 16), float32] */;\n",
      "    %45 = nn.bias_add(%44, %p217) /* ty=Tensor[(1, 64, 16, 16), float32] */;\n",
      "    nn.relu(%45) /* ty=Tensor[(1, 64, 16, 16), float32] */\n",
      "  };\n",
      "  %47 = %46(%input, %onnx::Conv_193, %onnx::Conv_194) /* ty=Tensor[(1, 64, 16, 16), float32] */;\n",
      "  %48 = fn (%p019: Tensor[(1, 64, 16, 16), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    nn.max_pool2d(%p019, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %49 = %48(%47) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %50 = fn (%p018: Tensor[(1, 64, 8, 8), float32], %p116: Tensor[(64, 64, 3, 3), float32], %p216: Tensor[(64), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %42 = nn.conv2d(%p018, %p116, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %43 = nn.bias_add(%42, %p216) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%43) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %51 = %50(%49, %onnx::Conv_196, %onnx::Conv_197) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %52 = fn (%p017: Tensor[(1, 64, 8, 8), float32], %p115: Tensor[(64, 64, 3, 3), float32], %p215: Tensor[(64), float32], %p37: Tensor[(1, 64, 8, 8), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %39 = nn.conv2d(%p017, %p115, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %40 = nn.bias_add(%39, %p215) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %41 = add(%40, %p37) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%41) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %53 = %52(%51, %onnx::Conv_199, %onnx::Conv_200, %49) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %54 = fn (%p016: Tensor[(1, 64, 8, 8), float32], %p114: Tensor[(64, 64, 3, 3), float32], %p214: Tensor[(64), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %37 = nn.conv2d(%p016, %p114, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %38 = nn.bias_add(%37, %p214) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%38) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %55 = %54(%53, %onnx::Conv_202, %onnx::Conv_203) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %56 = fn (%p015: Tensor[(1, 64, 8, 8), float32], %p113: Tensor[(64, 64, 3, 3), float32], %p213: Tensor[(64), float32], %p36: Tensor[(1, 64, 8, 8), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %34 = nn.conv2d(%p015, %p113, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %35 = nn.bias_add(%34, %p213) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %36 = add(%35, %p36) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%36) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %57 = %56(%55, %onnx::Conv_205, %onnx::Conv_206, %53) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %58 = fn (%p014: Tensor[(1, 64, 8, 8), float32], %p112: Tensor[(128, 64, 3, 3), float32], %p212: Tensor[(128), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %32 = nn.conv2d(%p014, %p112, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %33 = nn.bias_add(%32, %p212) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%33) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %60 = fn (%p021: Tensor[(1, 64, 8, 8), float32], %p118: Tensor[(128, 64, 1, 1), float32], %p218: Tensor[(128), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %59 = nn.conv2d(%p021, %p118, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.bias_add(%59, %p218) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %61 = %58(%57, %onnx::Conv_208, %onnx::Conv_209) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %62 = %60(%57, %onnx::Conv_214, %onnx::Conv_215) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %63 = fn (%p013: Tensor[(1, 128, 4, 4), float32], %p111: Tensor[(128, 128, 3, 3), float32], %p211: Tensor[(128), float32], %p35: Tensor[(1, 128, 4, 4), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %29 = nn.conv2d(%p013, %p111, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %30 = nn.bias_add(%29, %p211) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %31 = add(%30, %p35) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%31) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %64 = %63(%61, %onnx::Conv_211, %onnx::Conv_212, %62) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %65 = fn (%p012: Tensor[(1, 128, 4, 4), float32], %p110: Tensor[(128, 128, 3, 3), float32], %p210: Tensor[(128), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %27 = nn.conv2d(%p012, %p110, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %28 = nn.bias_add(%27, %p210) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%28) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %66 = %65(%64, %onnx::Conv_217, %onnx::Conv_218) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %67 = fn (%p011: Tensor[(1, 128, 4, 4), float32], %p19: Tensor[(128, 128, 3, 3), float32], %p29: Tensor[(128), float32], %p34: Tensor[(1, 128, 4, 4), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %24 = nn.conv2d(%p011, %p19, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %25 = nn.bias_add(%24, %p29) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %26 = add(%25, %p34) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%26) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %68 = %67(%66, %onnx::Conv_220, %onnx::Conv_221, %64) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %69 = fn (%p010: Tensor[(1, 128, 4, 4), float32], %p18: Tensor[(256, 128, 3, 3), float32], %p28: Tensor[(256), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %22 = nn.conv2d(%p010, %p18, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %23 = nn.bias_add(%22, %p28) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%23) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %71 = fn (%p022: Tensor[(1, 128, 4, 4), float32], %p119: Tensor[(256, 128, 1, 1), float32], %p219: Tensor[(256), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %70 = nn.conv2d(%p022, %p119, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.bias_add(%70, %p219) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %72 = %69(%68, %onnx::Conv_223, %onnx::Conv_224) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %73 = %71(%68, %onnx::Conv_229, %onnx::Conv_230) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %74 = fn (%p09: Tensor[(1, 256, 2, 2), float32], %p17: Tensor[(256, 256, 3, 3), float32], %p27: Tensor[(256), float32], %p33: Tensor[(1, 256, 2, 2), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %19 = nn.conv2d(%p09, %p17, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %20 = nn.bias_add(%19, %p27) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %21 = add(%20, %p33) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%21) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %75 = %74(%72, %onnx::Conv_226, %onnx::Conv_227, %73) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %76 = fn (%p08: Tensor[(1, 256, 2, 2), float32], %p16: Tensor[(256, 256, 3, 3), float32], %p26: Tensor[(256), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %17 = nn.conv2d(%p08, %p16, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %18 = nn.bias_add(%17, %p26) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%18) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %77 = %76(%75, %onnx::Conv_232, %onnx::Conv_233) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %78 = fn (%p07: Tensor[(1, 256, 2, 2), float32], %p15: Tensor[(256, 256, 3, 3), float32], %p25: Tensor[(256), float32], %p32: Tensor[(1, 256, 2, 2), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %14 = nn.conv2d(%p07, %p15, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %15 = nn.bias_add(%14, %p25) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %16 = add(%15, %p32) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%16) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %79 = %78(%77, %onnx::Conv_235, %onnx::Conv_236, %75) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %80 = fn (%p06: Tensor[(1, 256, 2, 2), float32], %p14: Tensor[(512, 256, 3, 3), float32], %p24: Tensor[(512), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %12 = nn.conv2d(%p06, %p14, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %13 = nn.bias_add(%12, %p24) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%13) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %82 = fn (%p023: Tensor[(1, 256, 2, 2), float32], %p120: Tensor[(512, 256, 1, 1), float32], %p220: Tensor[(512), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %81 = nn.conv2d(%p023, %p120, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.bias_add(%81, %p220) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %83 = %80(%79, %onnx::Conv_238, %onnx::Conv_239) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %84 = %82(%79, %onnx::Conv_244, %onnx::Conv_245) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %85 = fn (%p05: Tensor[(1, 512, 1, 1), float32], %p13: Tensor[(512, 512, 3, 3), float32], %p23: Tensor[(512), float32], %p31: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %9 = nn.conv2d(%p05, %p13, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %10 = nn.bias_add(%9, %p23) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %11 = add(%10, %p31) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%11) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %86 = %85(%83, %onnx::Conv_241, %onnx::Conv_242, %84) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %87 = fn (%p04: Tensor[(1, 512, 1, 1), float32], %p12: Tensor[(512, 512, 3, 3), float32], %p22: Tensor[(512), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %7 = nn.conv2d(%p04, %p12, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %8 = nn.bias_add(%7, %p22) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%8) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %88 = %87(%86, %onnx::Conv_247, %onnx::Conv_248) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %89 = fn (%p03: Tensor[(1, 512, 1, 1), float32], %p11: Tensor[(512, 512, 3, 3), float32], %p21: Tensor[(512), float32], %p3: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %4 = nn.conv2d(%p03, %p11, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %5 = nn.bias_add(%4, %p21) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %6 = add(%5, %p3) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%6) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %90 = %89(%88, %onnx::Conv_250, %onnx::Conv_251, %86) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %91 = fn (%p02: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    nn.global_avg_pool2d(%p02) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %92 = %91(%90) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %93 = fn (%p01: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512), float32] {\n",
      "    %3 = nn.batch_flatten(%p01) /* ty=Tensor[(1, 512), float32] */;\n",
      "    nn.batch_flatten(%3) /* ty=Tensor[(1, 512), float32] */\n",
      "  };\n",
      "  %94 = %93(%92) /* ty=Tensor[(1, 512), float32] */;\n",
      "  %95 = fn (%p0: Tensor[(1, 512), float32], %p1: Tensor[(1000, 512), float32], %p2: Tensor[(1000), float32], Primitive=1) -> Tensor[(1, 1000), float32] {\n",
      "    %0 = multiply(1f /* ty=float32 */, %p2) /* ty=Tensor[(1000), float32] */;\n",
      "    %1 = nn.dense(%p0, %p1, units=1000) /* ty=Tensor[(1, 1000), float32] */;\n",
      "    %2 = expand_dims(%0, axis=0) /* ty=Tensor[(1, 1000), float32] */;\n",
      "    add(%1, %2) /* ty=Tensor[(1, 1000), float32] */\n",
      "  };\n",
      "  %95(%94, %fc.weight, %fc.bias) /* ty=Tensor[(1, 1000), float32] */\n",
      "}\n",
      "\n",
      "Updated IR Module saved to updated_ir_module.json\n",
      "Inference Output: [[  1.1991472    1.5156441    8.383804     4.1304545    3.8710587\n",
      "    3.351125    12.3856       0.89964104  -2.6876838   -0.7517101\n",
      "  -12.122135   -11.241267   -12.009333   -11.295452   -10.903672\n",
      "  -11.607884   -12.0306     -11.834351   -11.971089   -10.807476\n",
      "  -10.851832   -11.770303   -11.015853   -13.330586   -11.497328\n",
      "  -11.572263   -12.518873   -11.901098   -10.727928   -12.872775\n",
      "  -10.735834   -11.95736    -12.174752   -10.084704    -9.54368\n",
      "  -11.506999   -10.538146   -10.738209   -11.903496   -10.549879\n",
      "  -10.760906   -11.159172   -11.616862   -10.6440325  -11.755911\n",
      "  -11.051258   -10.778828   -12.690132   -10.567687   -10.45058\n",
      "  -10.071108   -10.158873   -11.530322   -11.24294    -10.890731\n",
      "  -10.354391   -10.532895   -10.326235   -11.368172   -11.353812\n",
      "  -11.01524    -10.722385   -10.0382395  -11.240724   -10.827199\n",
      "  -10.446142   -12.382098   -10.649864   -11.215791    -9.916964\n",
      "  -11.168485   -10.302932   -10.194185   -10.695862   -10.594373\n",
      "   -9.969687   -10.032689   -11.254231   -10.340916   -10.426459\n",
      "  -13.404969   -11.120693   -12.311246   -12.560176   -12.567278\n",
      "  -10.27837    -13.121917   -11.32082    -11.198563   -10.877364\n",
      "  -12.626449   -13.280139   -10.953911   -13.211042   -10.337468\n",
      "  -12.094887   -11.656877   -10.252504   -11.336841   -11.527584\n",
      "  -11.726592   -11.117761   -11.470993   -11.202996   -11.525417\n",
      "  -10.480901   -10.639101    -8.979923   -11.017828   -10.442428\n",
      "  -11.004472   -10.53439    -10.358673   -11.444066   -11.314023\n",
      "  -12.06948    -12.026245   -11.452458   -10.04379    -11.374828\n",
      "  -10.083724   -10.2297     -10.931335   -10.037307    -9.929464\n",
      "  -10.213905   -11.629546   -12.449455   -11.889868   -10.991748\n",
      "  -12.523232   -11.50233    -11.258036   -12.479238   -12.51074\n",
      "  -11.804939   -12.379719   -11.192828   -12.276123   -10.757187\n",
      "   -9.918565   -10.962289   -10.633095   -11.244456   -11.650437\n",
      "  -10.700925    -9.871685    -9.725882    -9.874868   -10.439568\n",
      "  -10.423969   -11.932411   -11.953508   -11.087061   -10.723437\n",
      "  -10.627464   -11.082892   -11.69705    -11.659439   -12.177437\n",
      "  -12.722758   -10.77822    -10.184649   -11.906837   -11.4907465\n",
      "  -12.01379    -11.317411   -12.191057   -11.023503   -10.831224\n",
      "  -11.301554    -9.551787   -10.696487   -11.173869   -11.368213\n",
      "  -12.248409   -12.638984   -12.134263   -10.340657   -10.96266\n",
      "  -10.293807   -11.147244   -11.178097   -11.691045   -10.797666\n",
      "  -12.201542   -12.1171875  -11.557047   -11.412893   -10.978652\n",
      "  -11.585171   -11.275923   -11.7281475  -11.811866   -12.667297\n",
      "  -10.344428   -11.344688   -12.437691   -11.504845   -11.847511\n",
      "  -12.595014   -11.06918    -12.378915   -10.67122    -10.742865\n",
      "  -12.380669   -11.800008   -11.230043   -12.302099    -9.77252\n",
      "  -11.180407   -11.363852   -11.501486   -10.865619   -13.272425\n",
      "  -10.734725   -11.699269   -10.754809   -11.16135    -11.66649\n",
      "  -11.856873   -11.066535   -12.589387   -11.209201   -12.181125\n",
      "  -11.97669    -12.788313   -12.217023   -12.419635   -10.901372\n",
      "   -9.837566   -10.939768   -11.544492   -13.075183   -12.454419\n",
      "  -12.197575   -11.784683   -11.810679   -11.224535   -11.923293\n",
      "  -12.080843   -11.057723   -10.125732   -11.851459   -13.372319\n",
      "  -10.684416   -11.814974   -10.765247   -11.057712   -10.950403\n",
      "  -10.737324   -10.058976   -12.982531   -10.7704935  -11.132464\n",
      "  -13.255425   -12.48826    -10.553372   -10.200834   -11.352975\n",
      "  -11.291986   -10.551704   -13.647771   -10.243978   -10.808711\n",
      "  -11.496542   -12.081955   -12.069645   -12.072678   -11.539718\n",
      "  -11.482014   -11.757157   -11.261179   -11.282379   -13.519809\n",
      "  -12.4653015  -12.0075865  -12.813206   -13.29528    -11.453511\n",
      "  -12.48486    -11.069268   -10.602276   -10.638871   -11.033113\n",
      "  -12.17832    -11.291893   -12.08031    -11.463825   -11.855741\n",
      "  -11.772335   -11.97222    -11.547969   -11.947699   -11.561091\n",
      "  -11.77668    -10.794461   -12.989262   -12.272521   -12.456982\n",
      "  -11.832416   -12.244606   -11.219613   -10.67097    -11.168589\n",
      "  -10.868429   -10.856115   -11.887053   -11.137196   -11.407443\n",
      "  -11.318388   -10.960263   -11.576029   -10.95354     -9.920632\n",
      "  -10.488922   -11.540815   -11.667669   -10.809218   -10.603803\n",
      "  -12.211688   -11.9456415  -13.302124   -11.927772   -10.960994\n",
      "  -11.4192915  -11.94439     -9.915958   -10.398366   -10.656316\n",
      "  -11.8214855  -12.28118    -11.245601   -10.31931    -11.828881\n",
      "  -12.870643   -12.10013    -11.347849   -10.927776   -11.510698\n",
      "  -12.563591   -11.903496   -11.128015   -10.853421   -10.253602\n",
      "  -10.813449   -11.10634    -12.085528   -12.892809   -13.171221\n",
      "  -12.366309   -13.636316   -12.100655   -13.109832   -12.834781\n",
      "  -13.480509   -11.220943   -12.569112   -11.802153   -11.573852\n",
      "  -10.96167    -11.130839   -10.980959   -10.602485   -12.186253\n",
      "  -11.108518   -11.8640785  -12.457883   -13.718247   -13.14337\n",
      "  -11.8268175  -13.098402   -11.961426   -11.62366    -11.895601\n",
      "  -12.351338   -12.353757   -12.288267   -13.13418    -12.693674\n",
      "  -13.292653   -12.992123   -12.346941   -12.223157   -14.127526\n",
      "  -10.723177   -10.360267   -13.1458845  -11.358171   -11.374305\n",
      "   -9.96931     -9.88134    -10.897896   -10.671423    -9.942137\n",
      "  -11.102171    -9.895482    -9.382171   -10.288231   -11.453684\n",
      "  -11.4775715  -10.4029      -9.662297   -10.547914   -10.775878\n",
      "  -11.493397   -10.924563   -11.486054   -10.847458   -10.29691\n",
      "  -11.130326   -10.07323    -10.398518   -10.723439   -10.152922\n",
      "  -11.005896   -11.606291   -10.763351   -10.98896     -9.121198\n",
      "  -10.566554   -11.593145   -10.642011   -11.03565    -11.012441\n",
      "  -10.973948    -9.988645   -11.808231   -10.694451   -10.489194\n",
      "  -10.065179   -11.770257    -9.604569    -9.692443   -11.09264\n",
      "  -10.621056   -12.477817    -9.828447   -10.478068   -12.65984\n",
      "  -10.222085    -9.158461   -10.178403   -10.196749   -11.881558\n",
      "   -9.049179   -10.330581    -9.74402    -10.358798   -11.32592\n",
      "  -11.259414    -9.281102   -10.5577     -10.808662   -10.747463\n",
      "  -10.624609   -10.508365    -9.377975   -10.464652   -10.1934595\n",
      "   -9.022194    -9.4816885  -10.221032   -10.595457   -10.000006\n",
      "  -11.838903   -10.319635   -10.778088   -10.410101   -11.316443\n",
      "   -9.763775   -11.086419   -10.082845   -11.190767    -9.022071\n",
      "  -10.111295   -10.67404    -11.73931     -9.614825   -11.35441\n",
      "   -9.981626   -10.326764    -9.793337   -10.380736   -10.411726\n",
      "  -10.274521   -10.613165    -9.9963     -10.812789   -10.580374\n",
      "   -9.533745   -11.066174   -10.59196    -11.166496   -10.625605\n",
      "  -11.242233    -9.919355   -10.211074    -9.412439   -10.257844\n",
      "  -10.546214   -10.598891    -9.986193   -10.5227375  -10.926456\n",
      "  -10.153763   -12.14548    -10.383433   -10.055192   -10.757936\n",
      "  -10.830407   -11.116202   -11.563007    -9.351349   -10.446954\n",
      "  -10.335064   -11.536481   -11.510438   -10.160795   -12.200104\n",
      "  -12.013741   -10.07029    -11.374665    -9.057842    -9.730281\n",
      "  -10.798821   -11.163661   -10.589594   -10.30576    -10.260701\n",
      "  -10.767707   -10.283653   -11.101129    -9.50624    -11.351921\n",
      "  -12.344481   -10.616684   -11.209386   -10.810422   -11.023119\n",
      "  -11.137479   -10.872887   -10.425126   -11.320463   -10.666471\n",
      "  -10.083987   -10.849169   -11.422681   -11.1385765   -9.956027\n",
      "  -10.837244   -11.275486    -9.804121   -10.400077   -10.304225\n",
      "  -11.247654   -11.553861   -10.380091   -10.710128   -10.059998\n",
      "   -9.656888   -10.557738    -9.691091   -10.6390505  -11.773785\n",
      "  -11.209591    -9.021059   -10.569649   -11.691304   -11.94997\n",
      "  -10.473887   -10.943613   -10.511679   -10.060238   -10.173936\n",
      "  -10.603481   -10.131575    -9.844881   -10.42253    -11.088276\n",
      "  -12.307494   -10.66661    -11.240281   -10.628051   -11.2611065\n",
      "  -10.342757   -12.839908   -10.318283   -10.74573    -10.070682\n",
      "  -11.812175   -10.178648   -11.30042    -10.88984     -9.939494\n",
      "  -11.743765    -9.652233    -8.419736   -10.082763    -9.948399\n",
      "  -11.094197   -11.113031   -11.482252   -12.809471   -10.208055\n",
      "   -9.567387    -9.092108   -10.779869    -9.916003   -12.431049\n",
      "   -9.8555565   -9.5150385  -10.690231   -10.08466    -11.011001\n",
      "   -9.53558    -11.491962   -11.754435   -10.331877   -10.377055\n",
      "  -10.428086   -11.611412   -11.228121   -11.126154   -11.097856\n",
      "   -9.955946   -10.346287   -10.913315   -10.169021   -10.355752\n",
      "   -8.864121    -9.9358835  -10.481401   -10.768552   -13.296839\n",
      "  -11.621116    -9.9313135  -10.479207    -9.249334    -9.835624\n",
      "   -9.051737   -10.551243   -10.143201    -9.638621   -10.411894\n",
      "  -11.522141   -11.6923895  -10.972278   -10.50879    -11.388469\n",
      "  -10.041752   -10.725027   -10.415303   -11.928786   -11.267654\n",
      "   -9.743171   -11.203084   -10.909858    -9.488365   -11.03038\n",
      "  -11.697199   -12.262422   -10.978585   -10.957789   -10.3217745\n",
      "  -11.00912    -11.945021    -9.732458   -10.929658   -10.447778\n",
      "  -10.99083    -12.088908   -10.90619     -9.590361    -9.832176\n",
      "  -11.353622   -11.616703   -11.432258   -10.356353    -9.747823\n",
      "  -11.013215   -10.446239   -10.656376   -10.703383    -9.784826\n",
      "  -10.63505    -10.923194   -10.66834    -11.400369   -11.780805\n",
      "  -11.930633   -10.257454    -9.660589    -9.478232   -10.320635\n",
      "  -10.280121    -9.778127   -10.980263   -10.400285   -11.608968\n",
      "  -10.683958   -10.573816   -12.521421   -11.936954   -10.343477\n",
      "  -10.600738   -10.861446    -9.799214   -12.447291    -9.46664\n",
      "   -9.576204    -9.877472   -10.343189   -12.303781    -9.800564\n",
      "  -11.540801   -11.491147   -12.135739   -10.034975    -9.180105\n",
      "  -10.049384   -10.435529    -9.1334     -10.06286    -10.404044\n",
      "   -9.963235   -10.135626   -10.845782   -10.189971   -13.273971\n",
      "  -10.857699    -9.689305   -10.391746   -10.142341   -11.989093\n",
      "   -9.381628    -9.500546    -9.552378   -12.263592   -12.727036\n",
      "  -10.231915   -11.21921    -10.3628     -11.581192   -10.825961\n",
      "  -11.084363    -9.577559   -10.033751   -10.086607   -10.797811\n",
      "  -10.760717   -10.862741    -9.742262   -12.418554   -10.722822\n",
      "  -11.598843   -11.8702545  -11.960229    -9.290556   -11.09286\n",
      "  -10.92985    -10.794142   -11.11309     -9.531809   -10.831511\n",
      "  -11.19938    -11.698993   -10.165446   -10.905284    -9.936567\n",
      "   -9.614837   -10.399042    -9.971232   -10.155599    -9.602895\n",
      "  -10.396256    -9.939725    -9.922484    -9.827392   -10.272031\n",
      "  -10.839178   -11.5269165  -10.935283   -11.347639    -9.7062\n",
      "  -10.630711   -11.406731    -9.811561   -10.143679   -12.234791\n",
      "  -11.467449    -9.790218    -9.288122    -9.383594   -10.978273\n",
      "   -9.806336   -10.8887     -11.4418955  -11.366417   -10.655192\n",
      "   -9.355869    -8.956748   -10.940451   -10.7985525   -9.562183\n",
      "   -9.108223    -9.637179   -11.659261   -10.544793   -11.205489\n",
      "  -12.431804   -10.680933   -10.689919    -9.667178    -9.938016\n",
      "  -10.296721   -10.008396   -11.272291   -11.248133   -10.878893\n",
      "  -12.645858   -11.2090645  -10.094208    -9.145064   -10.082887\n",
      "  -13.147959    -9.881802   -11.129582    -9.759003   -10.693994\n",
      "  -10.93202    -11.451532   -10.7410755   -9.581199    -9.662592\n",
      "  -10.9242735   -9.435294    -9.3704815   -9.913051    -9.864184\n",
      "  -10.72704    -10.251485    -8.8706     -10.704698   -10.39215\n",
      "   -9.44209    -10.518647   -11.935101   -10.8748045  -10.408203\n",
      "  -10.512722   -10.075176   -11.028189   -11.452331   -10.622638\n",
      "   -9.87895    -13.4881115  -10.579449   -10.65972    -10.406524\n",
      "  -11.479616   -11.692576   -11.331147   -11.543866   -11.844403\n",
      "  -12.005193   -11.634156   -10.820128   -10.380681   -10.163724\n",
      "  -10.375196   -10.611568   -10.634927   -10.642405   -12.497501\n",
      "   -9.48696    -10.721193   -10.562489   -12.930438    -9.646634\n",
      "  -10.621136   -11.567894    -9.862231   -11.228434   -11.096434\n",
      "  -11.300393   -10.088118   -10.829233   -11.425788   -10.052712\n",
      "   -9.782451   -10.710214    -9.6294      -9.675962   -11.33458\n",
      "  -10.894172   -10.193315   -10.512574    -9.677175   -10.197803\n",
      "  -10.781165   -10.172862   -10.091463    -9.821992   -12.004243\n",
      "  -11.760875   -10.0679865  -10.089651    -9.446833   -10.094588\n",
      "  -10.156468   -10.744105   -14.066587    -9.7076435  -11.689818\n",
      "  -11.684495   -11.259899   -10.422908   -11.032163   -11.108632\n",
      "   -9.86193    -11.141087   -11.476546   -10.534477   -10.973114\n",
      "  -10.684293   -10.748719    -9.746156    -9.478253    -9.813074\n",
      "  -11.562045   -10.510761    -9.925166   -10.145973   -10.647989\n",
      "  -10.136328   -11.733613   -10.954734   -10.620647   -11.934334\n",
      "  -11.658416   -11.389238   -10.865116   -11.763795   -10.985409\n",
      "  -10.142093   -11.755171   -11.324326   -10.786146   -10.177671\n",
      "  -10.190279   -10.546099   -11.690166    -9.847539   -10.346571\n",
      "  -10.558024   -11.904755    -9.712469   -11.186144   -11.075877\n",
      "   -9.474722   -10.047368   -11.007625    -9.796386   -12.12353\n",
      "  -11.156205    -9.999205   -10.405908   -11.816977   -11.079164\n",
      "  -10.752971   -10.19549    -11.155489    -9.951023   -10.152985\n",
      "  -10.68988     -9.833459    -9.151115    -9.512034   -11.008481\n",
      "   -9.702529   -11.532574   -10.294667    -9.02231    -10.908407\n",
      "  -12.028776   -10.813892   -10.77753    -11.454458   -11.5952425\n",
      "  -12.022697   -12.78819    -11.353153   -12.5493555  -12.073535\n",
      "  -12.275542   -13.076452   -10.922594   -11.531162    -9.3630905 ]]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.relay import transform\n",
    "from tvm.contrib import graph_executor\n",
    "\n",
    "onnx_model_path = \"model.onnx\"  \n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "shape_dict = {\"input\": [1, 3, 32, 32]}  \n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "print(\"Original IR Module:\")\n",
    "print(mod)\n",
    "\n",
    "mod = transform.InferType()(mod)\n",
    "mod = transform.SimplifyInference()(mod)\n",
    "\n",
    "mod = transform.FuseOps(fuse_opt_level=2)(mod)\n",
    "\n",
    "mod = transform.AlterOpLayout()(mod)\n",
    "\n",
    "print(\"\\nUpdated IR Module after transformations:\")\n",
    "print(mod)\n",
    "\n",
    "visualize_path = \"updated_ir_module.json\"\n",
    "with open(visualize_path, \"w\") as f:\n",
    "    f.write(mod.astext(show_meta_data=False))\n",
    "print(f\"Updated IR Module saved to {visualize_path}\")\n",
    "\n",
    "target = \"llvm\"  \n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "dev = tvm.device(target, 0)\n",
    "module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n",
    "import numpy as np\n",
    "input_data = np.random.rand(1, 3, 32, 32).astype(\"float32\")\n",
    "module.set_input(\"input\",input_data)  \n",
    "module.run()\n",
    "output = module.get_output(0).asnumpy()\n",
    "print(\"Inference Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%input: Tensor[(1, 3, 32, 32), float32], %fc.weight: Tensor[(1000, 512), float32], %fc.bias: Tensor[(1000), float32], %onnx::Conv_193: Tensor[(64, 3, 7, 7), float32], %onnx::Conv_194: Tensor[(64), float32], %onnx::Conv_196: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_197: Tensor[(64), float32], %onnx::Conv_199: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_200: Tensor[(64), float32], %onnx::Conv_202: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_203: Tensor[(64), float32], %onnx::Conv_205: Tensor[(64, 64, 3, 3), float32], %onnx::Conv_206: Tensor[(64), float32], %onnx::Conv_208: Tensor[(128, 64, 3, 3), float32], %onnx::Conv_209: Tensor[(128), float32], %onnx::Conv_211: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_212: Tensor[(128), float32], %onnx::Conv_214: Tensor[(128, 64, 1, 1), float32], %onnx::Conv_215: Tensor[(128), float32], %onnx::Conv_217: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_218: Tensor[(128), float32], %onnx::Conv_220: Tensor[(128, 128, 3, 3), float32], %onnx::Conv_221: Tensor[(128), float32], %onnx::Conv_223: Tensor[(256, 128, 3, 3), float32], %onnx::Conv_224: Tensor[(256), float32], %onnx::Conv_226: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_227: Tensor[(256), float32], %onnx::Conv_229: Tensor[(256, 128, 1, 1), float32], %onnx::Conv_230: Tensor[(256), float32], %onnx::Conv_232: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_233: Tensor[(256), float32], %onnx::Conv_235: Tensor[(256, 256, 3, 3), float32], %onnx::Conv_236: Tensor[(256), float32], %onnx::Conv_238: Tensor[(512, 256, 3, 3), float32], %onnx::Conv_239: Tensor[(512), float32], %onnx::Conv_241: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_242: Tensor[(512), float32], %onnx::Conv_244: Tensor[(512, 256, 1, 1), float32], %onnx::Conv_245: Tensor[(512), float32], %onnx::Conv_247: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_248: Tensor[(512), float32], %onnx::Conv_250: Tensor[(512, 512, 3, 3), float32], %onnx::Conv_251: Tensor[(512), float32]) -> Tensor[(1, 1000), float32] {\n",
      "  %46 = fn (%p020: Tensor[(1, 3, 32, 32), float32], %p117: Tensor[(64, 3, 7, 7), float32], %p217: Tensor[(64), float32], Primitive=1) -> Tensor[(1, 64, 16, 16), float32] {\n",
      "    %44 = nn.conv2d(%p020, %p117, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 16, 16), float32] */;\n",
      "    %45 = nn.bias_add(%44, %p217) /* ty=Tensor[(1, 64, 16, 16), float32] */;\n",
      "    nn.relu(%45) /* ty=Tensor[(1, 64, 16, 16), float32] */\n",
      "  };\n",
      "  %47 = %46(%input, %onnx::Conv_193, %onnx::Conv_194) /* ty=Tensor[(1, 64, 16, 16), float32] */;\n",
      "  %48 = fn (%p019: Tensor[(1, 64, 16, 16), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    nn.max_pool2d(%p019, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %49 = %48(%47) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %50 = fn (%p018: Tensor[(1, 64, 8, 8), float32], %p116: Tensor[(64, 64, 3, 3), float32], %p216: Tensor[(64), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %42 = nn.conv2d(%p018, %p116, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %43 = nn.bias_add(%42, %p216) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%43) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %51 = %50(%49, %onnx::Conv_196, %onnx::Conv_197) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %52 = fn (%p017: Tensor[(1, 64, 8, 8), float32], %p115: Tensor[(64, 64, 3, 3), float32], %p215: Tensor[(64), float32], %p37: Tensor[(1, 64, 8, 8), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %39 = nn.conv2d(%p017, %p115, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %40 = nn.bias_add(%39, %p215) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %41 = add(%40, %p37) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%41) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %53 = %52(%51, %onnx::Conv_199, %onnx::Conv_200, %49) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %54 = fn (%p016: Tensor[(1, 64, 8, 8), float32], %p114: Tensor[(64, 64, 3, 3), float32], %p214: Tensor[(64), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %37 = nn.conv2d(%p016, %p114, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %38 = nn.bias_add(%37, %p214) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%38) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %55 = %54(%53, %onnx::Conv_202, %onnx::Conv_203) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %56 = fn (%p015: Tensor[(1, 64, 8, 8), float32], %p113: Tensor[(64, 64, 3, 3), float32], %p213: Tensor[(64), float32], %p36: Tensor[(1, 64, 8, 8), float32], Primitive=1) -> Tensor[(1, 64, 8, 8), float32] {\n",
      "    %34 = nn.conv2d(%p015, %p113, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %35 = nn.bias_add(%34, %p213) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    %36 = add(%35, %p36) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "    nn.relu(%36) /* ty=Tensor[(1, 64, 8, 8), float32] */\n",
      "  };\n",
      "  %57 = %56(%55, %onnx::Conv_205, %onnx::Conv_206, %53) /* ty=Tensor[(1, 64, 8, 8), float32] */;\n",
      "  %58 = fn (%p014: Tensor[(1, 64, 8, 8), float32], %p112: Tensor[(128, 64, 3, 3), float32], %p212: Tensor[(128), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %32 = nn.conv2d(%p014, %p112, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %33 = nn.bias_add(%32, %p212) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%33) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %60 = fn (%p021: Tensor[(1, 64, 8, 8), float32], %p118: Tensor[(128, 64, 1, 1), float32], %p218: Tensor[(128), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %59 = nn.conv2d(%p021, %p118, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.bias_add(%59, %p218) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %61 = %58(%57, %onnx::Conv_208, %onnx::Conv_209) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %62 = %60(%57, %onnx::Conv_214, %onnx::Conv_215) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %63 = fn (%p013: Tensor[(1, 128, 4, 4), float32], %p111: Tensor[(128, 128, 3, 3), float32], %p211: Tensor[(128), float32], %p35: Tensor[(1, 128, 4, 4), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %29 = nn.conv2d(%p013, %p111, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %30 = nn.bias_add(%29, %p211) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %31 = add(%30, %p35) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%31) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %64 = %63(%61, %onnx::Conv_211, %onnx::Conv_212, %62) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %65 = fn (%p012: Tensor[(1, 128, 4, 4), float32], %p110: Tensor[(128, 128, 3, 3), float32], %p210: Tensor[(128), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %27 = nn.conv2d(%p012, %p110, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %28 = nn.bias_add(%27, %p210) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%28) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %66 = %65(%64, %onnx::Conv_217, %onnx::Conv_218) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %67 = fn (%p011: Tensor[(1, 128, 4, 4), float32], %p19: Tensor[(128, 128, 3, 3), float32], %p29: Tensor[(128), float32], %p34: Tensor[(1, 128, 4, 4), float32], Primitive=1) -> Tensor[(1, 128, 4, 4), float32] {\n",
      "    %24 = nn.conv2d(%p011, %p19, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %25 = nn.bias_add(%24, %p29) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    %26 = add(%25, %p34) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "    nn.relu(%26) /* ty=Tensor[(1, 128, 4, 4), float32] */\n",
      "  };\n",
      "  %68 = %67(%66, %onnx::Conv_220, %onnx::Conv_221, %64) /* ty=Tensor[(1, 128, 4, 4), float32] */;\n",
      "  %69 = fn (%p010: Tensor[(1, 128, 4, 4), float32], %p18: Tensor[(256, 128, 3, 3), float32], %p28: Tensor[(256), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %22 = nn.conv2d(%p010, %p18, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %23 = nn.bias_add(%22, %p28) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%23) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %71 = fn (%p022: Tensor[(1, 128, 4, 4), float32], %p119: Tensor[(256, 128, 1, 1), float32], %p219: Tensor[(256), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %70 = nn.conv2d(%p022, %p119, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.bias_add(%70, %p219) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %72 = %69(%68, %onnx::Conv_223, %onnx::Conv_224) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %73 = %71(%68, %onnx::Conv_229, %onnx::Conv_230) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %74 = fn (%p09: Tensor[(1, 256, 2, 2), float32], %p17: Tensor[(256, 256, 3, 3), float32], %p27: Tensor[(256), float32], %p33: Tensor[(1, 256, 2, 2), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %19 = nn.conv2d(%p09, %p17, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %20 = nn.bias_add(%19, %p27) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %21 = add(%20, %p33) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%21) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %75 = %74(%72, %onnx::Conv_226, %onnx::Conv_227, %73) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %76 = fn (%p08: Tensor[(1, 256, 2, 2), float32], %p16: Tensor[(256, 256, 3, 3), float32], %p26: Tensor[(256), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %17 = nn.conv2d(%p08, %p16, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %18 = nn.bias_add(%17, %p26) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%18) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %77 = %76(%75, %onnx::Conv_232, %onnx::Conv_233) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %78 = fn (%p07: Tensor[(1, 256, 2, 2), float32], %p15: Tensor[(256, 256, 3, 3), float32], %p25: Tensor[(256), float32], %p32: Tensor[(1, 256, 2, 2), float32], Primitive=1) -> Tensor[(1, 256, 2, 2), float32] {\n",
      "    %14 = nn.conv2d(%p07, %p15, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %15 = nn.bias_add(%14, %p25) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    %16 = add(%15, %p32) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "    nn.relu(%16) /* ty=Tensor[(1, 256, 2, 2), float32] */\n",
      "  };\n",
      "  %79 = %78(%77, %onnx::Conv_235, %onnx::Conv_236, %75) /* ty=Tensor[(1, 256, 2, 2), float32] */;\n",
      "  %80 = fn (%p06: Tensor[(1, 256, 2, 2), float32], %p14: Tensor[(512, 256, 3, 3), float32], %p24: Tensor[(512), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %12 = nn.conv2d(%p06, %p14, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %13 = nn.bias_add(%12, %p24) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%13) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %82 = fn (%p023: Tensor[(1, 256, 2, 2), float32], %p120: Tensor[(512, 256, 1, 1), float32], %p220: Tensor[(512), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %81 = nn.conv2d(%p023, %p120, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.bias_add(%81, %p220) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %83 = %80(%79, %onnx::Conv_238, %onnx::Conv_239) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %84 = %82(%79, %onnx::Conv_244, %onnx::Conv_245) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %85 = fn (%p05: Tensor[(1, 512, 1, 1), float32], %p13: Tensor[(512, 512, 3, 3), float32], %p23: Tensor[(512), float32], %p31: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %9 = nn.conv2d(%p05, %p13, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %10 = nn.bias_add(%9, %p23) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %11 = add(%10, %p31) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%11) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %86 = %85(%83, %onnx::Conv_241, %onnx::Conv_242, %84) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %87 = fn (%p04: Tensor[(1, 512, 1, 1), float32], %p12: Tensor[(512, 512, 3, 3), float32], %p22: Tensor[(512), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %7 = nn.conv2d(%p04, %p12, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %8 = nn.bias_add(%7, %p22) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%8) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %88 = %87(%86, %onnx::Conv_247, %onnx::Conv_248) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %89 = fn (%p03: Tensor[(1, 512, 1, 1), float32], %p11: Tensor[(512, 512, 3, 3), float32], %p21: Tensor[(512), float32], %p3: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    %4 = nn.conv2d(%p03, %p11, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %5 = nn.bias_add(%4, %p21) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    %6 = add(%5, %p3) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "    nn.relu(%6) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %90 = %89(%88, %onnx::Conv_250, %onnx::Conv_251, %86) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %91 = fn (%p02: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512, 1, 1), float32] {\n",
      "    nn.global_avg_pool2d(%p02) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  };\n",
      "  %92 = %91(%90) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %93 = fn (%p01: Tensor[(1, 512, 1, 1), float32], Primitive=1) -> Tensor[(1, 512), float32] {\n",
      "    %3 = nn.batch_flatten(%p01) /* ty=Tensor[(1, 512), float32] */;\n",
      "    nn.batch_flatten(%3) /* ty=Tensor[(1, 512), float32] */\n",
      "  };\n",
      "  %94 = %93(%92) /* ty=Tensor[(1, 512), float32] */;\n",
      "  %95 = fn (%p0: Tensor[(1, 512), float32], %p1: Tensor[(1000, 512), float32], %p2: Tensor[(1000), float32], Primitive=1) -> Tensor[(1, 1000), float32] {\n",
      "    %0 = multiply(1f /* ty=float32 */, %p2) /* ty=Tensor[(1000), float32] */;\n",
      "    %1 = nn.dense(%p0, %p1, units=1000) /* ty=Tensor[(1, 1000), float32] */;\n",
      "    %2 = expand_dims(%0, axis=0) /* ty=Tensor[(1, 1000), float32] */;\n",
      "    add(%1, %2) /* ty=Tensor[(1, 1000), float32] */\n",
      "  };\n",
      "  %95(%94, %fc.weight, %fc.bias) /* ty=Tensor[(1, 1000), float32] */\n",
      "}\n",
      "\n",
      "8496171243901758301\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import graph_executor\n",
    "\n",
    "# Load and build your model\n",
    "# mod, params = relay.testing.mlp.get_workload(batch_size=1)\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=\"llvm\", params=params)\n",
    "\n",
    "# Visualize IR\n",
    "print(mod)\n",
    "# Export to DOT (Graphviz)\n",
    "dot_graph = tvm.ir.structural_hash(mod)\n",
    "print(dot_graph)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
