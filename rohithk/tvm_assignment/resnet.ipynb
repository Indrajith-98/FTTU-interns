{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohithk/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rohithk/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root='./data', train=True,transform=train_transforms, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data',train=False, transform=test_transforms, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/50], Loss: 1.6223\n",
      "Epoch: [2/50], Loss: 0.7003\n",
      "Epoch: [3/50], Loss: 0.5203\n",
      "Epoch: [4/50], Loss: 0.3901\n",
      "Epoch: [5/50], Loss: 0.2898\n",
      "Epoch: [6/50], Loss: 0.2260\n",
      "Epoch: [7/50], Loss: 0.1784\n",
      "Epoch: [8/50], Loss: 0.1360\n",
      "Epoch: [9/50], Loss: 0.1112\n",
      "Epoch: [10/50], Loss: 0.0990\n",
      "Epoch: [11/50], Loss: 0.0884\n",
      "Epoch: [12/50], Loss: 0.0827\n",
      "Epoch: [13/50], Loss: 0.0705\n",
      "Epoch: [14/50], Loss: 0.0675\n",
      "Epoch: [15/50], Loss: 0.0610\n",
      "Epoch: [16/50], Loss: 0.0591\n",
      "Epoch: [17/50], Loss: 0.0534\n",
      "Epoch: [18/50], Loss: 0.0488\n",
      "Epoch: [19/50], Loss: 0.0481\n",
      "Epoch: [20/50], Loss: 0.0464\n",
      "Epoch: [21/50], Loss: 0.0441\n",
      "Epoch: [22/50], Loss: 0.0418\n",
      "Epoch: [23/50], Loss: 0.0393\n",
      "Epoch: [24/50], Loss: 0.0363\n",
      "Epoch: [25/50], Loss: 0.0359\n",
      "Epoch: [26/50], Loss: 0.0398\n",
      "Epoch: [27/50], Loss: 0.0313\n",
      "Epoch: [28/50], Loss: 0.0313\n",
      "Epoch: [29/50], Loss: 0.0306\n",
      "Epoch: [30/50], Loss: 0.0314\n",
      "Epoch: [31/50], Loss: 0.0292\n",
      "Epoch: [32/50], Loss: 0.0283\n",
      "Epoch: [33/50], Loss: 0.0268\n",
      "Epoch: [34/50], Loss: 0.0278\n",
      "Epoch: [35/50], Loss: 0.0233\n",
      "Epoch: [36/50], Loss: 0.0244\n",
      "Epoch: [37/50], Loss: 0.0239\n",
      "Epoch: [38/50], Loss: 0.0250\n",
      "Epoch: [39/50], Loss: 0.0257\n",
      "Epoch: [40/50], Loss: 0.0235\n",
      "Epoch: [41/50], Loss: 0.0183\n",
      "Epoch: [42/50], Loss: 0.0263\n",
      "Epoch: [43/50], Loss: 0.0196\n",
      "Epoch: [44/50], Loss: 0.0258\n",
      "Epoch: [45/50], Loss: 0.0188\n",
      "Epoch: [46/50], Loss: 0.0249\n",
      "Epoch: [47/50], Loss: 0.0201\n",
      "Epoch: [48/50], Loss: 0.0126\n",
      "Epoch: [49/50], Loss: 0.0262\n",
      "Epoch: [50/50], Loss: 0.0150\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    r_loss = 0.0\n",
    "    for images,labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        r_loss += loss\n",
    "    print(f\"Epoch: [{epoch + 1}/{epochs}], Loss: {r_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model saved to model.pth\n",
      "ONNX model saved to model.onnx\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model_path = \"model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"PyTorch model saved to {model_path}\")\n",
    "\n",
    "dummy_input = torch.randn(1, 3,32,32)  # Batch size of 1, input size of 10\n",
    "model.eval()\n",
    "# Convert to ONNX\n",
    "onnx_path = \"model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,            # Model to be converted\n",
    "    dummy_input,# Dummy input tensor\n",
    "    onnx_path,               # Output ONNX file path\n",
    "    export_params=True,      # Store the trained parameter weights inside the model\n",
    "    opset_version=11,        # ONNX opset version to export to\n",
    "    do_constant_folding=True, # Optimize constant folding for inference\n",
    "    input_names=[\"input\"],   # Input tensor name\n",
    "    output_names=[\"output\"], # Output tensor name\n",
    "    \n",
    ")\n",
    "\n",
    "print(f\"ONNX model saved to {onnx_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
