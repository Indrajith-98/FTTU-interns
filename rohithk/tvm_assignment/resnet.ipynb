{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohithk/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rohithk/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root='./data', train=True,transform=train_transforms, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data',train=False, transform=test_transforms, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/50], Loss: 1.6291\n",
      "Epoch: [2/50], Loss: 0.7003\n",
      "Epoch: [3/50], Loss: 0.5189\n",
      "Epoch: [4/50], Loss: 0.3976\n",
      "Epoch: [5/50], Loss: 0.2989\n",
      "Epoch: [6/50], Loss: 0.2256\n",
      "Epoch: [7/50], Loss: 0.1742\n",
      "Epoch: [8/50], Loss: 0.1396\n",
      "Epoch: [9/50], Loss: 0.1205\n",
      "Epoch: [10/50], Loss: 0.1031\n",
      "Epoch: [11/50], Loss: 0.0874\n",
      "Epoch: [12/50], Loss: 0.0815\n",
      "Epoch: [13/50], Loss: 0.0698\n",
      "Epoch: [14/50], Loss: 0.0650\n",
      "Epoch: [15/50], Loss: 0.0592\n",
      "Epoch: [16/50], Loss: 0.0593\n",
      "Epoch: [17/50], Loss: 0.0530\n",
      "Epoch: [18/50], Loss: 0.0510\n",
      "Epoch: [19/50], Loss: 0.0483\n",
      "Epoch: [20/50], Loss: 0.0481\n",
      "Epoch: [21/50], Loss: 0.0477\n",
      "Epoch: [22/50], Loss: 0.0421\n",
      "Epoch: [23/50], Loss: 0.0393\n",
      "Epoch: [24/50], Loss: 0.0380\n",
      "Epoch: [25/50], Loss: 0.0343\n",
      "Epoch: [26/50], Loss: 0.0329\n",
      "Epoch: [27/50], Loss: 0.0342\n",
      "Epoch: [28/50], Loss: 0.0322\n",
      "Epoch: [29/50], Loss: 0.0331\n",
      "Epoch: [30/50], Loss: 0.0284\n",
      "Epoch: [31/50], Loss: 0.0304\n",
      "Epoch: [32/50], Loss: 0.0300\n",
      "Epoch: [33/50], Loss: 0.0229\n",
      "Epoch: [34/50], Loss: 0.0305\n",
      "Epoch: [35/50], Loss: 0.0243\n",
      "Epoch: [36/50], Loss: 0.0297\n",
      "Epoch: [37/50], Loss: 0.0250\n",
      "Epoch: [38/50], Loss: 0.0209\n",
      "Epoch: [39/50], Loss: 0.0219\n",
      "Epoch: [40/50], Loss: 0.0243\n",
      "Epoch: [41/50], Loss: 0.0230\n",
      "Epoch: [42/50], Loss: 0.0193\n",
      "Epoch: [43/50], Loss: 0.0194\n",
      "Epoch: [44/50], Loss: 0.0214\n",
      "Epoch: [45/50], Loss: 0.0199\n",
      "Epoch: [46/50], Loss: 0.0200\n",
      "Epoch: [47/50], Loss: 0.0196\n",
      "Epoch: [48/50], Loss: 0.0201\n",
      "Epoch: [49/50], Loss: 0.0218\n",
      "Epoch: [50/50], Loss: 0.0168\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    r_loss = 0.0\n",
    "    for images,labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        r_loss += loss\n",
    "    print(f\"Epoch: [{epoch + 1}/{epochs}], Loss: {r_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model saved to model.pth\n",
      "ONNX model saved to model.onnx\n"
     ]
    }
   ],
   "source": [
    "model_path = \"model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"PyTorch model saved to {model_path}\")\n",
    "\n",
    "dummy_input = torch.randn(64, 3,32,32)  # Batch size of 1, input size of 10\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "# Convert to ONNX\n",
    "onnx_path = \"model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,            # Model to be converted\n",
    "    dummy_input,# Dummy input tensor\n",
    "    onnx_path,               # Output ONNX file path\n",
    "    export_params=True,      # Store the trained parameter weights inside the model\n",
    "    opset_version=11,        # ONNX opset version to export to\n",
    "    do_constant_folding=True, # Optimize constant folding for inference\n",
    "    input_names=[\"input\"],   # Input tensor name\n",
    "    output_names=[\"output\"], # Output tensor name\n",
    "    dynamic_axes={           # Declare dimensions that may change\n",
    "        \"input\": {0: \"batch_size\"},  # Variable batch size\n",
    "        \"output\": {0: \"batch_size\"}  # Variable batch size\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"ONNX model saved to {onnx_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
